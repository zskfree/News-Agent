"""
RSSè®¢é˜…æºè¯»å–æ¨¡å—

è¯¥æ¨¡å—æä¾›äº†ä»RSSè®¢é˜…æºè¯»å–æ•°æ®çš„åŠŸèƒ½ã€‚
"""

import feedparser
import gc
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
import os
from collections import defaultdict
import hashlib
import json


def read_rss_feed(url):
    """
    ä»ç»™å®šçš„ URL è¯»å– RSS è®¢é˜…æºå¹¶è¿”å›æ¡ç›®åˆ—è¡¨ã€‚
    
    å‚æ•°:
        url (str): RSS è®¢é˜…æºçš„ URLã€‚
        
    è¿”å›:
        list: RSS è®¢é˜…æºä¸­çš„æ¡ç›®åˆ—è¡¨ã€‚
    """
    try:
        # è®¾ç½®è¶…æ—¶å’Œç”¨æˆ·ä»£ç†
        feed = feedparser.parse(url, agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        if hasattr(feed, 'entries') and feed.entries:
            entries = list(feed.entries)  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…å†…å­˜å¼•ç”¨é—®é¢˜
            del feed  # æ˜¾å¼åˆ é™¤feedå¯¹è±¡
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            return entries
        else:
            print(f"RSSè®¢é˜…æº {url} æ²¡æœ‰æ¡ç›®æˆ–æ ¼å¼ä¸æ­£ç¡®")
            return []
    except Exception as e:
        print(f"è¯»å– RSS è®¢é˜…æº {url} æ—¶å‡ºé”™: {e}")
        return []

def display_feed_entries(entries):
    """
    æ˜¾ç¤º RSS è®¢é˜…æºä¸­çš„æ¡ç›®ã€‚
    
    å‚æ•°:
        entries (list): RSS è®¢é˜…æºä¸­çš„æ¡ç›®åˆ—è¡¨ã€‚
    """
    if not entries:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¡ç›®ã€‚")
        return
    
    for i, entry in enumerate(entries):
        try:
            title = getattr(entry, 'title', 'æ— æ ‡é¢˜')
            link = getattr(entry, 'link', '#')
            published = getattr(entry, 'published', 'æœªçŸ¥æ—¶é—´')
            print(f"æ ‡é¢˜: {title}\né“¾æ¥: {link}\nå‘å¸ƒæ—¶é—´: {published}\n")
        except Exception as e:
            print(f"å¤„ç†ç¬¬ {i+1} ä¸ªæ¡ç›®æ—¶å‡ºé”™: {e}")
            continue

def get_recent_articles_summary(rss_urls, hours_limit=24, output_file=None, rss_sources=None):
    """
    è¯»å–æŒ‡å®šRSSè®¢é˜…æºåˆ—è¡¨ä¸­çš„æ‰€æœ‰è®¢é˜…æºï¼Œè·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ç« ä¿¡æ¯ï¼Œ
    å¹¶ç”ŸæˆMarkdownæ ¼å¼çš„æ±‡æ€»æŠ¥å‘Šã€‚
    
    å‚æ•°:
        rss_urls (list): RSSè®¢é˜…æºURLåˆ—è¡¨
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        output_file (str, optional): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜åˆ°æ–‡ä»¶
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯ï¼Œç”¨äºè·å–æºåç§°
        
    è¿”å›:
        str: Markdownæ ¼å¼çš„æ±‡æ€»æŠ¥å‘Š
    """
    # è®¡ç®—æ—¶é—´æˆªæ­¢ç‚¹
    cutoff_time = datetime.now() - timedelta(hours=hours_limit)
    
    # å­˜å‚¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ç« 
    all_articles = []
    
    print(f"å¼€å§‹è¯»å– {len(rss_urls)} ä¸ªRSSè®¢é˜…æº...")
    print(f"ç­›é€‰æ¡ä»¶ï¼šæœ€è¿‘ {hours_limit} å°æ—¶å†…çš„æ–‡ç« ")
    print("-" * 50)
    
    for i, url in enumerate(rss_urls, 1):
        print(f"[{i}/{len(rss_urls)}] æ­£åœ¨è¯»å–: {url}")
        
        try:
            # è¯»å–RSSè®¢é˜…æº
            entries = read_rss_feed(url)
            
            if not entries:
                print(f"  -> æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
                continue
                
            # ç­›é€‰æœ€è¿‘çš„æ–‡ç« 
            recent_entries = []
            for entry in entries:
                try:
                    # å°è¯•è§£æå‘å¸ƒæ—¶é—´
                    published_time = None
                    
                    # å°è¯•å¤šç§æ—¶é—´å­—æ®µ
                    for time_field in ['published', 'updated', 'created']:
                        if hasattr(entry, time_field):
                            time_str = getattr(entry, time_field)
                            try:
                                # å°è¯•ä½¿ç”¨ parsedate_to_datetime è§£æ
                                published_time = parsedate_to_datetime(time_str)
                                break
                            except (TypeError, ValueError):
                                # å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                                try:
                                    # å°è¯•è§£æ struct_time æ ¼å¼
                                    if hasattr(entry, f'{time_field}_parsed'):
                                        time_struct = getattr(entry, f'{time_field}_parsed')
                                        if time_struct:
                                            published_time = datetime(*time_struct[:6])
                                            break
                                except:
                                    continue
                    
                    # å¦‚æœæˆåŠŸè§£ææ—¶é—´ä¸”åœ¨æ—¶é—´èŒƒå›´å†…
                    if published_time and published_time > cutoff_time:
                        article_info = {
                            'title': getattr(entry, 'title', 'æ— æ ‡é¢˜'),
                            'link': getattr(entry, 'link', '#'),
                            'published': published_time.strftime('%Y-%m-%d %H:%M'),
                            'source_url': url
                        }
                        recent_entries.append(article_info)
                        
                except Exception as e:
                    # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œä»ç„¶åŒ…å«æ–‡ç« ï¼ˆå‡è®¾æ˜¯æœ€è¿‘çš„ï¼‰
                    article_info = {
                        'title': getattr(entry, 'title', 'æ— æ ‡é¢˜'),
                        'link': getattr(entry, 'link', '#'),
                        'published': getattr(entry, 'published', 'æ—¶é—´æœªçŸ¥'),
                        'source_url': url
                    }
                    recent_entries.append(article_info)
                    continue
            
            all_articles.extend(recent_entries)
            print(f"  -> æ‰¾åˆ° {len(recent_entries)} ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
            
            # æ¸…ç†å†…å­˜
            del entries
            gc.collect()
            
        except Exception as e:
            print(f"  -> è¯»å–å¤±è´¥: {e}")
            continue
    
    print("-" * 50)
    print(f"æ±‡æ€»å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    try:
        all_articles.sort(key=lambda x: datetime.strptime(x['published'], '%Y-%m-%d %H:%M') 
                         if x['published'] != 'æ—¶é—´æœªçŸ¥' else datetime.min, reverse=True)
    except:
        # å¦‚æœæ’åºå¤±è´¥ï¼Œä¿æŒåŸé¡ºåº
        pass
    
    # ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
    markdown_content = generate_markdown_report(all_articles, hours_limit, rss_sources)
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return markdown_content

def generate_markdown_report(articles, hours_limit, rss_sources=None):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        articles (list): æ–‡ç« ä¿¡æ¯åˆ—è¡¨
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯ï¼Œç”¨äºè·å–æºåç§°å’Œç½‘ç«™URL
        
    è¿”å›:
        str: Markdownæ ¼å¼çš„æŠ¥å‘Š
    """
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    # æ„å»ºMarkdownå†…å®¹
    md_lines = [
        f"# æ–°é—»æ±‡æ€»æŠ¥å‘Š",
        f"",
        f"**ç”Ÿæˆæ—¶é—´**: {current_time}",
        f"**æ—¶é—´èŒƒå›´**: æœ€è¿‘ {hours_limit} å°æ—¶",
        f"**æ–‡ç« æ€»æ•°**: {len(articles)}",
        f"",
        f"---",
        f""
    ]
    
    if not articles:
        md_lines.append("**æš‚æ— ç¬¦åˆæ¡ä»¶çš„æ–‡ç« **")
    else:
        # æŒ‰æ¥æºåˆ†ç»„
        sources = {}
        for article in articles:
            source_url = article['source_url']
            if source_url not in sources:
                sources[source_url] = []
            sources[source_url].append(article)
        
        # ç”Ÿæˆæ¯ä¸ªæ¥æºçš„æ–‡ç« åˆ—è¡¨
        for source_url, source_articles in sources.items():
            # è·å–æºåç§°å’Œç½‘ç«™URL
            source_name = source_url  # é»˜è®¤ä½¿ç”¨RSS URLä½œä¸ºåç§°
            website_url = source_url  # é»˜è®¤ä½¿ç”¨RSS URLä½œä¸ºé“¾æ¥
            
            if rss_sources:
                # ä»rss_sourcesä¸­æŸ¥æ‰¾å¯¹åº”çš„é…ç½®
                for source in rss_sources:
                    if source.get('rss') == source_url:  # æ³¨æ„è¿™é‡Œåº”è¯¥æ˜¯'rss'å­—æ®µ
                        source_name = source.get('name', source_url)
                        # ä¼˜å…ˆä½¿ç”¨websiteå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨urlå­—æ®µï¼Œæœ€åä½¿ç”¨RSS URL
                        website_url = source.get('website') or source.get('url') or source_url
                        break
            
            # ç”Ÿæˆå¯ç‚¹å‡»çš„æ¥æºæ ‡é¢˜
            md_lines.append(f"## ğŸ“° æ¥æº: [{source_name}]({website_url})")
            md_lines.append(f"")
            
            for article in source_articles:
                title = article['title'].replace('[', '\\[').replace(']', '\\]')  # è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
                link = article['link']
                published = article['published']
                
                md_lines.append(f"### [{title}]({link})")
                md_lines.append(f"**å‘å¸ƒæ—¶é—´**: {published}")
                md_lines.append(f"")
            
            md_lines.append("---")
            md_lines.append("")
    
    return '\n'.join(md_lines)

def generate_news_by_categories(rss_sources, hours_limit=24, output_dir="news"):
    """
    æŒ‰åˆ†ç±»è¯»å–RSSè®¢é˜…æºï¼Œä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆç‹¬ç«‹çš„Markdownæ–°é—»æ±‡æ€»æŠ¥å‘Šã€‚
    
    å‚æ•°:
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯åˆ—è¡¨
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"news"
        
    è¿”å›:
        dict: æ¯ä¸ªåˆ†ç±»çš„æŠ¥å‘Šç”Ÿæˆç»“æœ {category: {"success": bool, "file_path": str, "article_count": int}}
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„RSSæº
    categories = defaultdict(list)
    for source in rss_sources:
        category = source.get('category', 'æœªåˆ†ç±»')
        categories[category].append(source)
    
    print(f"å‘ç° {len(categories)} ä¸ªåˆ†ç±»: {list(categories.keys())}")
    print("="*60)
    
    results = {}
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»ç”ŸæˆæŠ¥å‘Š
    for category, sources in categories.items():
        print(f"\nå¤„ç†åˆ†ç±»: {category} ({len(sources)} ä¸ªè®¢é˜…æº)")
        print("-" * 40)
        
        try:
            # æå–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰RSS URL
            rss_urls = [source.get('rss') for source in sources if source.get('rss')]
            
            if not rss_urls:
                print(f"  åˆ†ç±» '{category}' æ²¡æœ‰æœ‰æ•ˆçš„RSSè®¢é˜…æº")
                results[category] = {
                    "success": False, 
                    "file_path": None, 
                    "article_count": 0,
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„RSSè®¢é˜…æº"
                }
                continue
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_category = category.replace(' ', '_').replace('/', '_').replace('\\', '_')
            timestamp = datetime.now().strftime('%Y%m%d')
            filename = f"{safe_category}_{timestamp}.md"
            output_file = os.path.join(output_dir, filename)
            
            # ç”Ÿæˆè¯¥åˆ†ç±»çš„æ–°é—»æ±‡æ€»
            markdown_content = get_recent_articles_summary(
                rss_urls=rss_urls,
                hours_limit=hours_limit,
                output_file=output_file,
                rss_sources=sources  # åªä¼ é€’è¯¥åˆ†ç±»çš„æºä¿¡æ¯
            )
            
            # ç»Ÿè®¡æ–‡ç« æ•°é‡
            article_count = markdown_content.count('### [') if markdown_content else 0
            
            results[category] = {
                "success": True,
                "file_path": output_file,
                "article_count": article_count,
                "source_count": len(sources)
            }
            
            print(f"  âœ“ åˆ†ç±» '{category}' æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            print(f"    æ–‡ä»¶: {output_file}")
            print(f"    æ–‡ç« æ•°: {article_count}")
            
        except Exception as e:
            print(f"  âœ— åˆ†ç±» '{category}' æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            results[category] = {
                "success": False,
                "file_path": None,
                "article_count": 0,
                "error": str(e)
            }
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(results, output_dir, hours_limit)
    
    return results

def generate_summary_report(results, output_dir, hours_limit):
    """
    ç”Ÿæˆå„åˆ†ç±»æ–°é—»æ±‡æ€»çš„æ€»è§ˆæŠ¥å‘Š
    
    å‚æ•°:
        results (dict): å„åˆ†ç±»çš„ç”Ÿæˆç»“æœ
        output_dir (str): è¾“å‡ºç›®å½•
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    summary_file = os.path.join(output_dir, f"summary_report_{timestamp}.md")
    
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    md_lines = [
        "# æ–°é—»æ±‡æ€»æ€»è§ˆæŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {current_time}",
        f"**æ—¶é—´èŒƒå›´**: æœ€è¿‘ {hours_limit} å°æ—¶",
        f"**åˆ†ç±»æ€»æ•°**: {len(results)}",
        "",
        "---",
        ""
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_articles = sum(r.get('article_count', 0) for r in results.values())
    successful_categories = sum(1 for r in results.values() if r.get('success', False))
    
    md_lines.extend([
        "## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯",
        "",
        f"- **æ–‡ç« æ€»æ•°**: {total_articles}",
        f"- **æˆåŠŸåˆ†ç±»**: {successful_categories}/{len(results)}",
        "",
        "---",
        ""
    ])
    
    # å„åˆ†ç±»è¯¦æƒ…
    md_lines.extend([
        "## ğŸ“‚ åˆ†ç±»è¯¦æƒ…",
        ""
    ])
    
    for category, result in results.items():
        if result.get('success', False):
            file_name = os.path.basename(result['file_path'])
            article_count = result.get('article_count', 0)
            source_count = result.get('source_count', 0)
            
            md_lines.extend([
                f"### âœ… {category}",
                f"- **æ–‡ç« æ•°é‡**: {article_count}",
                f"- **è®¢é˜…æºæ•°é‡**: {source_count}",
                f"- **æŠ¥å‘Šæ–‡ä»¶**: [{file_name}](./{file_name})",
                ""
            ])
        else:
            error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
            md_lines.extend([
                f"### âŒ {category}",
                f"- **çŠ¶æ€**: ç”Ÿæˆå¤±è´¥",
                f"- **é”™è¯¯**: {error_msg}",
                ""
            ])
    
    # ä¿å­˜æ€»è§ˆæŠ¥å‘Š
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_lines))
        print(f"\nğŸ“‹ æ€»è§ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}")
    except Exception as e:
        print(f"\nâŒ æ€»è§ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

def generate_all_categories_news(rss_sources, hours_limit=24, output_dir="news"):
    """
    ä¸€é”®ç”Ÿæˆæ‰€æœ‰åˆ†ç±»çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        config_file_path (str): RSSé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"news"
        
    è¿”å›:
        dict: ç”Ÿæˆç»“æœ
    """
    try:
        if not rss_sources:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•RSSè®¢é˜…æºé…ç½®")
            return {}
        
        print(f"ğŸ“š æˆåŠŸåŠ è½½ {len(rss_sources)} ä¸ªRSSè®¢é˜…æºé…ç½®")
        
        # æŒ‰åˆ†ç±»ç”Ÿæˆæ–°é—»æŠ¥å‘Š
        results = generate_news_by_categories(
            rss_sources=rss_sources,
            hours_limit=hours_limit,
            output_dir=output_dir
        )
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        successful = sum(1 for r in results.values() if r.get('success', False))
        total_articles = sum(r.get('article_count', 0) for r in results.values())
        
        print(f"  âœ… æˆåŠŸç”Ÿæˆ: {successful}/{len(results)} ä¸ªåˆ†ç±»")
        print(f"  ğŸ“° æ–‡ç« æ€»æ•°: {total_articles}")
        print(f"  ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–°é—»æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}


def generate_article_hash(title, link):
    """
    ç”Ÿæˆæ–‡ç« çš„å”¯ä¸€å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡
    
    å‚æ•°:
        title (str): æ–‡ç« æ ‡é¢˜
        link (str): æ–‡ç« é“¾æ¥
        
    è¿”å›:
        str: æ–‡ç« çš„å“ˆå¸Œå€¼
    """
    # ä½¿ç”¨æ ‡é¢˜å’Œé“¾æ¥çš„ç»„åˆç”Ÿæˆå“ˆå¸Œï¼Œç¡®ä¿å”¯ä¸€æ€§
    content = f"{title.strip()}{link.strip()}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()

def load_existing_articles(file_path):
    """
    ä»ç°æœ‰çš„MDæ–‡ä»¶ä¸­åŠ è½½å·²å­˜åœ¨çš„æ–‡ç« å“ˆå¸Œå€¼
    
    å‚æ•°:
        file_path (str): MDæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        set: å·²å­˜åœ¨æ–‡ç« çš„å“ˆå¸Œå€¼é›†åˆ
    """
    existing_hashes = set()
    
    if not os.path.exists(file_path):
        return existing_hashes
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–æ‰€æœ‰æ–‡ç« é“¾æ¥å’Œæ ‡é¢˜
        import re
        article_pattern = r'### \[(.+?)\]\((.+?)\)'
        articles = re.findall(article_pattern, content)
        
        for title, link in articles:
            # æ¸…ç†è½¬ä¹‰å­—ç¬¦
            clean_title = title.replace('\\[', '[').replace('\\]', ']').strip()
            clean_link = link.strip()
            article_hash = generate_article_hash(clean_title, clean_link)
            existing_hashes.add(article_hash)
            
    except Exception as e:
        print(f"åŠ è½½ç°æœ‰æ–‡ç« æ—¶å‡ºé”™: {e}")
    
    return existing_hashes

def get_all_historical_articles(rss_urls, output_file=None, rss_sources=None, max_articles_per_source=50):
    """
    è·å–æ‰€æœ‰RSSè®¢é˜…æºçš„å†å²æ–°é—»ï¼ˆä¸é™æ—¶é—´ï¼‰ï¼Œå¹¶å»é‡è¿½åŠ åˆ°ç´¯ç§¯æ–‡æ¡£ä¸­
    
    å‚æ•°:
        rss_urls (list): RSSè®¢é˜…æºURLåˆ—è¡¨
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯ï¼Œç”¨äºè·å–æºåç§°
        max_articles_per_source (int): æ¯ä¸ªæºæœ€å¤šè·å–çš„æ–‡ç« æ•°é‡ï¼Œé˜²æ­¢æ–‡ä»¶è¿‡å¤§
        
    è¿”å›:
        dict: åŒ…å«æ–°å¢æ–‡ç« ä¿¡æ¯å’Œç»Ÿè®¡æ•°æ®
    """
    print(f"å¼€å§‹è·å– {len(rss_urls)} ä¸ªRSSè®¢é˜…æºçš„å†å²æ–°é—»...")
    print(f"æ¯ä¸ªè®¢é˜…æºæœ€å¤šè·å– {max_articles_per_source} ç¯‡æ–‡ç« ")
    print("-" * 50)
    
    # åŠ è½½ç°æœ‰æ–‡ç« å“ˆå¸Œå€¼
    existing_hashes = set()
    if output_file:
        existing_hashes = load_existing_articles(output_file)
        print(f"å·²åŠ è½½ {len(existing_hashes)} ä¸ªç°æœ‰æ–‡ç« çš„å“ˆå¸Œå€¼")
    
    # å­˜å‚¨æ‰€æœ‰æ–°æ–‡ç« 
    new_articles = []
    duplicate_count = 0
    source_stats = {}
    
    for i, url in enumerate(rss_urls, 1):
        print(f"[{i}/{len(rss_urls)}] æ­£åœ¨è¯»å–: {url}")
        
        try:
            # è¯»å–RSSè®¢é˜…æº
            entries = read_rss_feed(url)
            
            if not entries:
                print(f"  -> æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
                source_stats[url] = {"new": 0, "duplicate": 0, "total": 0}
                continue
            
            # é™åˆ¶æ–‡ç« æ•°é‡
            entries = entries[:max_articles_per_source]
            
            new_count = 0
            dup_count = 0
            
            for entry in entries:
                try:
                    title = getattr(entry, 'title', 'æ— æ ‡é¢˜')
                    link = getattr(entry, 'link', '#')
                    
                    # ç”Ÿæˆæ–‡ç« å“ˆå¸Œå€¼
                    article_hash = generate_article_hash(title, link)
                    
                    # æ£€æŸ¥æ˜¯å¦é‡å¤
                    if article_hash in existing_hashes:
                        dup_count += 1
                        continue
                    
                    # è§£æå‘å¸ƒæ—¶é—´
                    published_time = None
                    for time_field in ['published', 'updated', 'created']:
                        if hasattr(entry, time_field):
                            time_str = getattr(entry, time_field)
                            try:
                                published_time = parsedate_to_datetime(time_str)
                                break
                            except (TypeError, ValueError):
                                try:
                                    if hasattr(entry, f'{time_field}_parsed'):
                                        time_struct = getattr(entry, f'{time_field}_parsed')
                                        if time_struct:
                                            published_time = datetime(*time_struct[:6])
                                            break
                                except:
                                    continue
                    
                    article_info = {
                        'title': title,
                        'link': link,
                        'published': published_time.strftime('%Y-%m-%d %H:%M') if published_time else 'æ—¶é—´æœªçŸ¥',
                        'source_url': url,
                        'hash': article_hash
                    }
                    
                    new_articles.append(article_info)
                    existing_hashes.add(article_hash)  # æ·»åŠ åˆ°å·²å­˜åœ¨é›†åˆä¸­
                    new_count += 1
                    
                except Exception as e:
                    print(f"  -> å¤„ç†æ–‡ç« æ—¶å‡ºé”™: {e}")
                    continue
            
            source_stats[url] = {
                "new": new_count, 
                "duplicate": dup_count, 
                "total": len(entries)
            }
            duplicate_count += dup_count
            
            print(f"  -> æ–°å¢ {new_count} ç¯‡ï¼Œé‡å¤ {dup_count} ç¯‡")
            
            # æ¸…ç†å†…å­˜
            del entries
            gc.collect()
            
        except Exception as e:
            print(f"  -> è¯»å–å¤±è´¥: {e}")
            source_stats[url] = {"new": 0, "duplicate": 0, "total": 0, "error": str(e)}
            continue
    
    print("-" * 50)
    print(f"æ±‡æ€»å®Œæˆï¼Œå…±æ‰¾åˆ° {len(new_articles)} ç¯‡æ–°æ–‡ç« ï¼Œè·³è¿‡ {duplicate_count} ç¯‡é‡å¤æ–‡ç« ")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    try:
        new_articles.sort(key=lambda x: datetime.strptime(x['published'], '%Y-%m-%d %H:%M') 
                         if x['published'] != 'æ—¶é—´æœªçŸ¥' else datetime.min, reverse=True)
    except:
        pass
    
    # å¦‚æœæœ‰æ–°æ–‡ç« ä¸”æŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œè¿½åŠ åˆ°æ–‡ä»¶
    if new_articles and output_file:
        append_articles_to_file(new_articles, output_file, rss_sources)
    
    return {
        'new_articles': new_articles,
        'new_count': len(new_articles),
        'duplicate_count': duplicate_count,
        'source_stats': source_stats
    }

def append_articles_to_file(new_articles, output_file, rss_sources=None):
    """
    å°†æ–°æ–‡ç« è¿½åŠ åˆ°ç°æœ‰çš„MDæ–‡ä»¶ä¸­
    
    å‚æ•°:
        new_articles (list): æ–°æ–‡ç« åˆ—è¡¨
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯
    """
    try:
        # è¯»å–ç°æœ‰å†…å®¹
        existing_content = ""
        if os.path.exists(output_file):
            with open(output_file, 'r', encoding='utf-8') as f:
                existing_content = f.read()
        
        # ç”Ÿæˆæ–°æ–‡ç« çš„Markdownå†…å®¹
        new_content = generate_cumulative_markdown_report(new_articles, rss_sources, existing_content)
        
        # å†™å…¥æ–‡ä»¶
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(new_content)
        
        print(f"æˆåŠŸè¿½åŠ  {len(new_articles)} ç¯‡æ–°æ–‡ç« åˆ°: {output_file}")
        
    except Exception as e:
        print(f"è¿½åŠ æ–‡ç« åˆ°æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def generate_cumulative_markdown_report(new_articles, rss_sources=None, existing_content=""):
    """
    ç”Ÿæˆç´¯ç§¯çš„Markdownæ ¼å¼æŠ¥å‘Š
    
    å‚æ•°:
        new_articles (list): æ–°æ–‡ç« åˆ—è¡¨
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯
        existing_content (str): ç°æœ‰æ–‡ä»¶å†…å®¹
        
    è¿”å›:
        str: æ›´æ–°åçš„Markdownå†…å®¹
    """
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    # å¦‚æœæ˜¯æ–°æ–‡ä»¶ï¼Œåˆ›å»ºå¤´éƒ¨
    if not existing_content:
        md_lines = [
            f"# ç´¯ç§¯æ–°é—»æ±‡æ€»",
            f"",
            f"**é¦–æ¬¡åˆ›å»ºæ—¶é—´**: {current_time}",
            f"**æœ€åæ›´æ–°æ—¶é—´**: {current_time}",
            f"",
            f"---",
            f""
        ]
    else:
        # æ›´æ–°ç°æœ‰æ–‡ä»¶çš„æœ€åæ›´æ–°æ—¶é—´
        import re
        md_lines = existing_content.split('\n')
        
        # æ›´æ–°æœ€åæ›´æ–°æ—¶é—´
        for i, line in enumerate(md_lines):
            if line.startswith('**æœ€åæ›´æ–°æ—¶é—´**:'):
                md_lines[i] = f"**æœ€åæ›´æ–°æ—¶é—´**: {current_time}"
                break
        else:
            # å¦‚æœæ‰¾ä¸åˆ°æ›´æ–°æ—¶é—´è¡Œï¼Œåœ¨ç¬¬5è¡Œæ’å…¥
            if len(md_lines) > 4:
                md_lines.insert(4, f"**æœ€åæ›´æ–°æ—¶é—´**: {current_time}")
    
    # å¦‚æœæœ‰æ–°æ–‡ç« ï¼Œæ·»åŠ åˆ°å¼€å¤´
    if new_articles:
        # æ‰¾åˆ°ç¬¬ä¸€ä¸ªæ–‡ç« å¼€å§‹çš„ä½ç½®ï¼ˆåœ¨---åé¢ï¼‰
        insert_pos = len(md_lines)
        for i, line in enumerate(md_lines):
            if line.strip() == "---" and i < len(md_lines) - 1:
                insert_pos = i + 2  # åœ¨---åé¢çš„ç©ºè¡Œåæ’å…¥
                break
        
        # æŒ‰æ¥æºåˆ†ç»„æ–°æ–‡ç« 
        sources = {}
        for article in new_articles:
            source_url = article['source_url']
            if source_url not in sources:
                sources[source_url] = []
            sources[source_url].append(article)
        
        # ç”Ÿæˆæ–°æ–‡ç« å†…å®¹
        new_lines = [f"## ğŸ†• æœ€æ–°æ›´æ–° ({current_time})"]
        
        for source_url, source_articles in sources.items():
            # è·å–æºåç§°å’Œç½‘ç«™URL
            source_name = source_url
            website_url = source_url
            
            if rss_sources:
                for source in rss_sources:
                    if source.get('rss') == source_url:
                        source_name = source.get('name', source_url)
                        website_url = source.get('website') or source.get('url') or source_url
                        break
            
            new_lines.append(f"### ğŸ“° æ¥æº: [{source_name}]({website_url})")
            new_lines.append("")
            
            for article in source_articles:
                title = article['title'].replace('[', '\\[').replace(']', '\\]')
                link = article['link']
                published = article['published']
                
                new_lines.append(f"#### [{title}]({link})")
                new_lines.append(f"**å‘å¸ƒæ—¶é—´**: {published}")
                new_lines.append("")
        
        new_lines.extend(["---", ""])
        
        # æ’å…¥æ–°å†…å®¹
        md_lines[insert_pos:insert_pos] = new_lines
    
    return '\n'.join(md_lines)

def generate_historical_news_by_categories(rss_sources, output_dir="cumulative_news", max_articles_per_source=50):
    """
    æŒ‰åˆ†ç±»è·å–å†å²æ–°é—»å¹¶è¿½åŠ åˆ°ç´¯ç§¯æ–‡æ¡£ä¸­
    
    å‚æ•°:
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯åˆ—è¡¨
        output_dir (str): è¾“å‡ºç›®å½•
        max_articles_per_source (int): æ¯ä¸ªæºæœ€å¤šè·å–çš„æ–‡ç« æ•°é‡
        
    è¿”å›:
        dict: æ¯ä¸ªåˆ†ç±»çš„å¤„ç†ç»“æœ
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„RSSæº
    categories = defaultdict(list)
    for source in rss_sources:
        category = source.get('category', 'æœªåˆ†ç±»')
        categories[category].append(source)
    
    print(f"å‘ç° {len(categories)} ä¸ªåˆ†ç±»: {list(categories.keys())}")
    print("="*60)
    
    results = {}
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»å¤„ç†æ–°é—»
    for category, sources in categories.items():
        print(f"\nå¤„ç†åˆ†ç±»: {category} ({len(sources)} ä¸ªè®¢é˜…æº)")
        print("-" * 40)
        
        try:
            # æå–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰RSS URL
            rss_urls = [source.get('rss') for source in sources if source.get('rss')]
            
            if not rss_urls:
                print(f"  åˆ†ç±» '{category}' æ²¡æœ‰æœ‰æ•ˆçš„RSSè®¢é˜…æº")
                results[category] = {
                    "success": False,
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„RSSè®¢é˜…æº",
                    "new_count": 0,
                    "duplicate_count": 0
                }
                continue
            
            # ç”Ÿæˆç´¯ç§¯æ–‡ä»¶å
            safe_category = category.replace(' ', '_').replace('/', '_').replace('\\', '_')
            filename = f"{safe_category}_cumulative.md"
            output_file = os.path.join(output_dir, filename)
            
            # è·å–å†å²æ–°é—»
            result = get_all_historical_articles(
                rss_urls=rss_urls,
                output_file=output_file,
                rss_sources=sources,
                max_articles_per_source=max_articles_per_source
            )
            
            results[category] = {
                "success": True,
                "file_path": output_file,
                "new_count": result['new_count'],
                "duplicate_count": result['duplicate_count'],
                "source_count": len(sources),
                "source_stats": result['source_stats']
            }
            
            print(f"  âœ“ åˆ†ç±» '{category}' å¤„ç†å®Œæˆ")
            print(f"    æ–‡ä»¶: {output_file}")
            print(f"    æ–°å¢æ–‡ç« : {result['new_count']}")
            print(f"    é‡å¤æ–‡ç« : {result['duplicate_count']}")
            
        except Exception as e:
            print(f"  âœ— åˆ†ç±» '{category}' å¤„ç†å¤±è´¥: {e}")
            results[category] = {
                "success": False,
                "error": str(e),
                "new_count": 0,
                "duplicate_count": 0
            }
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_cumulative_summary_report(results, output_dir)
    
    return results

def generate_cumulative_summary_report(results, output_dir):
    """
    ç”Ÿæˆç´¯ç§¯æ–°é—»çš„æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        results (dict): å„åˆ†ç±»çš„å¤„ç†ç»“æœ
        output_dir (str): è¾“å‡ºç›®å½•
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    summary_file = os.path.join(output_dir, f"cumulative_summary_{timestamp}.md")
    
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    md_lines = [
        "# ç´¯ç§¯æ–°é—»æ±‡æ€»æŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {current_time}",
        f"**åˆ†ç±»æ€»æ•°**: {len(results)}",
        "",
        "---",
        ""
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_new = sum(r.get('new_count', 0) for r in results.values())
    total_duplicate = sum(r.get('duplicate_count', 0) for r in results.values())
    successful_categories = sum(1 for r in results.values() if r.get('success', False))
    
    md_lines.extend([
        "## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯",
        "",
        f"- **æ–°å¢æ–‡ç« æ€»æ•°**: {total_new}",
        f"- **é‡å¤æ–‡ç« æ€»æ•°**: {total_duplicate}",
        f"- **æˆåŠŸåˆ†ç±»**: {successful_categories}/{len(results)}",
        "",
        "---",
        ""
    ])
    
    # å„åˆ†ç±»è¯¦æƒ…
    md_lines.extend([
        "## ğŸ“‚ åˆ†ç±»è¯¦æƒ…",
        ""
    ])
    
    for category, result in results.items():
        if result.get('success', False):
            file_name = os.path.basename(result['file_path'])
            new_count = result.get('new_count', 0)
            duplicate_count = result.get('duplicate_count', 0)
            source_count = result.get('source_count', 0)
            
            md_lines.extend([
                f"### âœ… {category}",
                f"- **æ–°å¢æ–‡ç« **: {new_count}",
                f"- **é‡å¤æ–‡ç« **: {duplicate_count}",
                f"- **è®¢é˜…æºæ•°é‡**: {source_count}",
                f"- **ç´¯ç§¯æ–‡ä»¶**: [{file_name}](./{file_name})",
                ""
            ])
        else:
            error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
            md_lines.extend([
                f"### âŒ {category}",
                f"- **çŠ¶æ€**: å¤„ç†å¤±è´¥",
                f"- **é”™è¯¯**: {error_msg}",
                ""
            ])
    
    # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_lines))
        print(f"\nğŸ“‹ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}")
    except Exception as e:
        print(f"\nâŒ æ±‡æ€»æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    try:
        # é€‰æ‹©è¿è¡Œæ¨¡å¼
        print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
        print("1. ç”Ÿæˆå•ä¸ªåˆ†ç±»çš„æ–°é—»æ±‡æ€» (å¦‚AIåˆ†ç±»)")
        print("2. ç”Ÿæˆæ‰€æœ‰åˆ†ç±»çš„æ–°é—»æ±‡æ€»")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
        
        if choice == "1":
            # åŸæœ‰çš„å•åˆ†ç±»æ¨¡å¼
            from load_rss_url import load_rss_sources, get_rss_urls_by_category
            RSS_CONFIG_FILE = r'RSS feed URL\rss_feed_url.json'
            rss_sources = load_rss_sources(RSS_CONFIG_FILE)

            # ç­›é€‰AIç›¸å…³çš„RSSè®¢é˜…æº
            ai_rss_urls = get_rss_urls_by_category(rss_sources, category='AI')

            if not ai_rss_urls:
                print("æ²¡æœ‰æ‰¾åˆ°AIç›¸å…³çš„RSSè®¢é˜…æºã€‚")
            else:
                # ç”Ÿæˆæœ€è¿‘24å°æ—¶çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š
                output_file = f"news_summary_{datetime.now().strftime('%Y%m%d')}.md"
                markdown_report = get_recent_articles_summary(
                    rss_urls=ai_rss_urls,
                    hours_limit=24,  # å¯ä»¥ä¿®æ”¹æ—¶é—´é™åˆ¶
                    output_file=output_file,
                    rss_sources=rss_sources  # ä¼ é€’RSSæºä¿¡æ¯
                )
                
                print("\n" + "="*50)
                print("MarkdownæŠ¥å‘Šé¢„è§ˆ:")
                print("="*50)
                print(markdown_report[:500] + "..." if len(markdown_report) > 500 else markdown_report)
        
        elif choice == "2":
            # æ–°çš„å¤šåˆ†ç±»æ¨¡å¼
            print("\nğŸš€ å¼€å§‹ç”Ÿæˆæ‰€æœ‰åˆ†ç±»çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š...")
            
            from load_rss_url import load_rss_sources
            # åŠ è½½RSSè®¢é˜…æºé…ç½®
            RSS_CONFIG_FILE = r'RSS feed URL\rss_feed_url.json'
            rss_sources = load_rss_sources(RSS_CONFIG_FILE)
            
            # å¯ä»¥è‡ªå®šä¹‰å‚æ•°
            hours_limit = 24  # æ—¶é—´é™åˆ¶
            timestamp = datetime.now().strftime('%Y%m%d')
            output_dir = f"news/{timestamp}"  # è¾“å‡ºç›®å½•

            results = generate_all_categories_news(
                rss_sources=rss_sources,
                hours_limit=hours_limit,
                output_dir=output_dir
            )
            
            if results:
                print(f"\nâœ¨ æ‰€æœ‰æŠ¥å‘Šå·²ç”Ÿæˆå®Œæˆï¼Œè¯·æŸ¥çœ‹ '{output_dir}' ç›®å½•")
            else:
                print("\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        
        else:
            print("æ— æ•ˆçš„é€‰æ‹©ï¼Œç¨‹åºé€€å‡ºã€‚")
                
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œæ—¶å‡ºé”™: {e}")
    finally:
        gc.collect()
