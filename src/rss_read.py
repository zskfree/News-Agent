"""
RSSè®¢é˜…æºè¯»å–æ¨¡å—

è¯¥æ¨¡å—æä¾›äº†ä»RSSè®¢é˜…æºè¯»å–æ•°æ®çš„åŠŸèƒ½ã€‚
"""

import feedparser
import gc
from datetime import datetime, timedelta
from email.utils import parsedate_to_datetime
import os
from collections import defaultdict


def read_rss_feed(url):
    """
    ä»ç»™å®šçš„ URL è¯»å– RSS è®¢é˜…æºå¹¶è¿”å›æ¡ç›®åˆ—è¡¨ã€‚
    
    å‚æ•°:
        url (str): RSS è®¢é˜…æºçš„ URLã€‚
        
    è¿”å›:
        list: RSS è®¢é˜…æºä¸­çš„æ¡ç›®åˆ—è¡¨ã€‚
    """
    try:
        # è®¾ç½®è¶…æ—¶å’Œç”¨æˆ·ä»£ç†
        feed = feedparser.parse(url, agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        if hasattr(feed, 'entries') and feed.entries:
            entries = list(feed.entries)  # åˆ›å»ºå‰¯æœ¬ä»¥é¿å…å†…å­˜å¼•ç”¨é—®é¢˜
            del feed  # æ˜¾å¼åˆ é™¤feedå¯¹è±¡
            gc.collect()  # å¼ºåˆ¶åƒåœ¾å›æ”¶
            return entries
        else:
            print(f"RSSè®¢é˜…æº {url} æ²¡æœ‰æ¡ç›®æˆ–æ ¼å¼ä¸æ­£ç¡®")
            return []
    except Exception as e:
        print(f"è¯»å– RSS è®¢é˜…æº {url} æ—¶å‡ºé”™: {e}")
        return []

def display_feed_entries(entries):
    """
    æ˜¾ç¤º RSS è®¢é˜…æºä¸­çš„æ¡ç›®ã€‚
    
    å‚æ•°:
        entries (list): RSS è®¢é˜…æºä¸­çš„æ¡ç›®åˆ—è¡¨ã€‚
    """
    if not entries:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ¡ç›®ã€‚")
        return
    
    for i, entry in enumerate(entries):
        try:
            title = getattr(entry, 'title', 'æ— æ ‡é¢˜')
            link = getattr(entry, 'link', '#')
            published = getattr(entry, 'published', 'æœªçŸ¥æ—¶é—´')
            print(f"æ ‡é¢˜: {title}\né“¾æ¥: {link}\nå‘å¸ƒæ—¶é—´: {published}\n")
        except Exception as e:
            print(f"å¤„ç†ç¬¬ {i+1} ä¸ªæ¡ç›®æ—¶å‡ºé”™: {e}")
            continue

def get_recent_articles_summary(rss_urls, hours_limit=24, output_file=None, rss_sources=None):
    """
    è¯»å–æŒ‡å®šRSSè®¢é˜…æºåˆ—è¡¨ä¸­çš„æ‰€æœ‰è®¢é˜…æºï¼Œè·å–æŒ‡å®šæ—¶é—´èŒƒå›´å†…çš„æ–‡ç« ä¿¡æ¯ï¼Œ
    å¹¶ç”ŸæˆMarkdownæ ¼å¼çš„æ±‡æ€»æŠ¥å‘Šã€‚
    
    å‚æ•°:
        rss_urls (list): RSSè®¢é˜…æºURLåˆ—è¡¨
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        output_file (str, optional): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœæä¾›åˆ™ä¿å­˜åˆ°æ–‡ä»¶
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯ï¼Œç”¨äºè·å–æºåç§°
        
    è¿”å›:
        str: Markdownæ ¼å¼çš„æ±‡æ€»æŠ¥å‘Š
    """
    # è®¡ç®—æ—¶é—´æˆªæ­¢ç‚¹
    cutoff_time = datetime.now() - timedelta(hours=hours_limit)
    
    # å­˜å‚¨æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ç« 
    all_articles = []
    
    print(f"å¼€å§‹è¯»å– {len(rss_urls)} ä¸ªRSSè®¢é˜…æº...")
    print(f"ç­›é€‰æ¡ä»¶ï¼šæœ€è¿‘ {hours_limit} å°æ—¶å†…çš„æ–‡ç« ")
    print("-" * 50)
    
    for i, url in enumerate(rss_urls, 1):
        print(f"[{i}/{len(rss_urls)}] æ­£åœ¨è¯»å–: {url}")
        
        try:
            # è¯»å–RSSè®¢é˜…æº
            entries = read_rss_feed(url)
            
            if not entries:
                print(f"  -> æ²¡æœ‰æ‰¾åˆ°æ–‡ç« ")
                continue
                
            # ç­›é€‰æœ€è¿‘çš„æ–‡ç« 
            recent_entries = []
            for entry in entries:
                try:
                    # å°è¯•è§£æå‘å¸ƒæ—¶é—´
                    published_time = None
                    
                    # å°è¯•å¤šç§æ—¶é—´å­—æ®µ
                    for time_field in ['published', 'updated', 'created']:
                        if hasattr(entry, time_field):
                            time_str = getattr(entry, time_field)
                            try:
                                # å°è¯•ä½¿ç”¨ parsedate_to_datetime è§£æ
                                published_time = parsedate_to_datetime(time_str)
                                break
                            except (TypeError, ValueError):
                                # å¦‚æœå¤±è´¥ï¼Œå°è¯•å…¶ä»–æ ¼å¼
                                try:
                                    # å°è¯•è§£æ struct_time æ ¼å¼
                                    if hasattr(entry, f'{time_field}_parsed'):
                                        time_struct = getattr(entry, f'{time_field}_parsed')
                                        if time_struct:
                                            published_time = datetime(*time_struct[:6])
                                            break
                                except:
                                    continue
                    
                    # å¦‚æœæˆåŠŸè§£ææ—¶é—´ä¸”åœ¨æ—¶é—´èŒƒå›´å†…
                    if published_time and published_time > cutoff_time:
                        article_info = {
                            'title': getattr(entry, 'title', 'æ— æ ‡é¢˜'),
                            'link': getattr(entry, 'link', '#'),
                            'published': published_time.strftime('%Y-%m-%d %H:%M'),
                            'source_url': url
                        }
                        recent_entries.append(article_info)
                        
                except Exception as e:
                    # å¦‚æœæ—¶é—´è§£æå¤±è´¥ï¼Œä»ç„¶åŒ…å«æ–‡ç« ï¼ˆå‡è®¾æ˜¯æœ€è¿‘çš„ï¼‰
                    article_info = {
                        'title': getattr(entry, 'title', 'æ— æ ‡é¢˜'),
                        'link': getattr(entry, 'link', '#'),
                        'published': getattr(entry, 'published', 'æ—¶é—´æœªçŸ¥'),
                        'source_url': url
                    }
                    recent_entries.append(article_info)
                    continue
            
            all_articles.extend(recent_entries)
            print(f"  -> æ‰¾åˆ° {len(recent_entries)} ç¯‡ç¬¦åˆæ¡ä»¶çš„æ–‡ç« ")
            
            # æ¸…ç†å†…å­˜
            del entries
            gc.collect()
            
        except Exception as e:
            print(f"  -> è¯»å–å¤±è´¥: {e}")
            continue
    
    print("-" * 50)
    print(f"æ±‡æ€»å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_articles)} ç¯‡æ–‡ç« ")
    
    # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
    try:
        all_articles.sort(key=lambda x: datetime.strptime(x['published'], '%Y-%m-%d %H:%M') 
                         if x['published'] != 'æ—¶é—´æœªçŸ¥' else datetime.min, reverse=True)
    except:
        # å¦‚æœæ’åºå¤±è´¥ï¼Œä¿æŒåŸé¡ºåº
        pass
    
    # ç”ŸæˆMarkdownæ ¼å¼æŠ¥å‘Š
    markdown_content = generate_markdown_report(all_articles, hours_limit, rss_sources)
    
    # å¦‚æœæŒ‡å®šäº†è¾“å‡ºæ–‡ä»¶ï¼Œä¿å­˜åˆ°æ–‡ä»¶
    if output_file:
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            print(f"æŠ¥å‘Šå·²ä¿å­˜åˆ°: {output_file}")
        except Exception as e:
            print(f"ä¿å­˜æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    return markdown_content

def generate_markdown_report(articles, hours_limit, rss_sources=None):
    """
    ç”ŸæˆMarkdownæ ¼å¼çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        articles (list): æ–‡ç« ä¿¡æ¯åˆ—è¡¨
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯ï¼Œç”¨äºè·å–æºåç§°å’Œç½‘ç«™URL
        
    è¿”å›:
        str: Markdownæ ¼å¼çš„æŠ¥å‘Š
    """
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    # æ„å»ºMarkdownå†…å®¹
    md_lines = [
        f"# æ–°é—»æ±‡æ€»æŠ¥å‘Š",
        f"",
        f"**ç”Ÿæˆæ—¶é—´**: {current_time}",
        f"**æ—¶é—´èŒƒå›´**: æœ€è¿‘ {hours_limit} å°æ—¶",
        f"**æ–‡ç« æ€»æ•°**: {len(articles)}",
        f"",
        f"---",
        f""
    ]
    
    if not articles:
        md_lines.append("**æš‚æ— ç¬¦åˆæ¡ä»¶çš„æ–‡ç« **")
    else:
        # æŒ‰æ¥æºåˆ†ç»„
        sources = {}
        for article in articles:
            source_url = article['source_url']
            if source_url not in sources:
                sources[source_url] = []
            sources[source_url].append(article)
        
        # ç”Ÿæˆæ¯ä¸ªæ¥æºçš„æ–‡ç« åˆ—è¡¨
        for source_url, source_articles in sources.items():
            # è·å–æºåç§°å’Œç½‘ç«™URL
            source_name = source_url  # é»˜è®¤ä½¿ç”¨RSS URLä½œä¸ºåç§°
            website_url = source_url  # é»˜è®¤ä½¿ç”¨RSS URLä½œä¸ºé“¾æ¥
            
            if rss_sources:
                # ä»rss_sourcesä¸­æŸ¥æ‰¾å¯¹åº”çš„é…ç½®
                for source in rss_sources:
                    if source.get('rss') == source_url:  # æ³¨æ„è¿™é‡Œåº”è¯¥æ˜¯'rss'å­—æ®µ
                        source_name = source.get('name', source_url)
                        # ä¼˜å…ˆä½¿ç”¨websiteå­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨urlå­—æ®µï¼Œæœ€åä½¿ç”¨RSS URL
                        website_url = source.get('website') or source.get('url') or source_url
                        break
            
            # ç”Ÿæˆå¯ç‚¹å‡»çš„æ¥æºæ ‡é¢˜
            md_lines.append(f"## ğŸ“° æ¥æº: [{source_name}]({website_url})")
            md_lines.append(f"")
            
            for article in source_articles:
                title = article['title'].replace('[', '\\[').replace(']', '\\]')  # è½¬ä¹‰Markdownç‰¹æ®Šå­—ç¬¦
                link = article['link']
                published = article['published']
                
                md_lines.append(f"### [{title}]({link})")
                md_lines.append(f"**å‘å¸ƒæ—¶é—´**: {published}")
                md_lines.append(f"")
            
            md_lines.append("---")
            md_lines.append("")
    
    return '\n'.join(md_lines)

def generate_news_by_categories(rss_sources, hours_limit=24, output_dir="news"):
    """
    æŒ‰åˆ†ç±»è¯»å–RSSè®¢é˜…æºï¼Œä¸ºæ¯ä¸ªåˆ†ç±»ç”Ÿæˆç‹¬ç«‹çš„Markdownæ–°é—»æ±‡æ€»æŠ¥å‘Šã€‚
    
    å‚æ•°:
        rss_sources (list): RSSæºé…ç½®ä¿¡æ¯åˆ—è¡¨
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"news"
        
    è¿”å›:
        dict: æ¯ä¸ªåˆ†ç±»çš„æŠ¥å‘Šç”Ÿæˆç»“æœ {category: {"success": bool, "file_path": str, "article_count": int}}
    """
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # æŒ‰åˆ†ç±»åˆ†ç»„RSSæº
    categories = defaultdict(list)
    for source in rss_sources:
        category = source.get('category', 'æœªåˆ†ç±»')
        categories[category].append(source)
    
    print(f"å‘ç° {len(categories)} ä¸ªåˆ†ç±»: {list(categories.keys())}")
    print("="*60)
    
    results = {}
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»ç”ŸæˆæŠ¥å‘Š
    for category, sources in categories.items():
        print(f"\nå¤„ç†åˆ†ç±»: {category} ({len(sources)} ä¸ªè®¢é˜…æº)")
        print("-" * 40)
        
        try:
            # æå–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰RSS URL
            rss_urls = [source.get('rss') for source in sources if source.get('rss')]
            
            if not rss_urls:
                print(f"  åˆ†ç±» '{category}' æ²¡æœ‰æœ‰æ•ˆçš„RSSè®¢é˜…æº")
                results[category] = {
                    "success": False, 
                    "file_path": None, 
                    "article_count": 0,
                    "error": "æ²¡æœ‰æœ‰æ•ˆçš„RSSè®¢é˜…æº"
                }
                continue
            
            # ç”Ÿæˆæ–‡ä»¶åï¼ˆå¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼‰
            safe_category = category.replace(' ', '_').replace('/', '_').replace('\\', '_')
            timestamp = datetime.now().strftime('%Y%m%d')
            filename = f"{safe_category}_{timestamp}.md"
            output_file = os.path.join(output_dir, filename)
            
            # ç”Ÿæˆè¯¥åˆ†ç±»çš„æ–°é—»æ±‡æ€»
            markdown_content = get_recent_articles_summary(
                rss_urls=rss_urls,
                hours_limit=hours_limit,
                output_file=output_file,
                rss_sources=sources  # åªä¼ é€’è¯¥åˆ†ç±»çš„æºä¿¡æ¯
            )
            
            # ç»Ÿè®¡æ–‡ç« æ•°é‡
            article_count = markdown_content.count('### [') if markdown_content else 0
            
            results[category] = {
                "success": True,
                "file_path": output_file,
                "article_count": article_count,
                "source_count": len(sources)
            }
            
            print(f"  âœ“ åˆ†ç±» '{category}' æŠ¥å‘Šç”ŸæˆæˆåŠŸ")
            print(f"    æ–‡ä»¶: {output_file}")
            print(f"    æ–‡ç« æ•°: {article_count}")
            
        except Exception as e:
            print(f"  âœ— åˆ†ç±» '{category}' æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            results[category] = {
                "success": False,
                "file_path": None,
                "article_count": 0,
                "error": str(e)
            }
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    generate_summary_report(results, output_dir, hours_limit)
    
    return results

def generate_summary_report(results, output_dir, hours_limit):
    """
    ç”Ÿæˆå„åˆ†ç±»æ–°é—»æ±‡æ€»çš„æ€»è§ˆæŠ¥å‘Š
    
    å‚æ•°:
        results (dict): å„åˆ†ç±»çš„ç”Ÿæˆç»“æœ
        output_dir (str): è¾“å‡ºç›®å½•
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰
    """
    timestamp = datetime.now().strftime('%Y%m%d_%H%M')
    summary_file = os.path.join(output_dir, f"summary_report_{timestamp}.md")
    
    current_time = datetime.now().strftime('%Y-%m-%d %H:%M')
    
    md_lines = [
        "# æ–°é—»æ±‡æ€»æ€»è§ˆæŠ¥å‘Š",
        "",
        f"**ç”Ÿæˆæ—¶é—´**: {current_time}",
        f"**æ—¶é—´èŒƒå›´**: æœ€è¿‘ {hours_limit} å°æ—¶",
        f"**åˆ†ç±»æ€»æ•°**: {len(results)}",
        "",
        "---",
        ""
    ]
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_articles = sum(r.get('article_count', 0) for r in results.values())
    successful_categories = sum(1 for r in results.values() if r.get('success', False))
    
    md_lines.extend([
        "## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯",
        "",
        f"- **æ–‡ç« æ€»æ•°**: {total_articles}",
        f"- **æˆåŠŸåˆ†ç±»**: {successful_categories}/{len(results)}",
        "",
        "---",
        ""
    ])
    
    # å„åˆ†ç±»è¯¦æƒ…
    md_lines.extend([
        "## ğŸ“‚ åˆ†ç±»è¯¦æƒ…",
        ""
    ])
    
    for category, result in results.items():
        if result.get('success', False):
            file_name = os.path.basename(result['file_path'])
            article_count = result.get('article_count', 0)
            source_count = result.get('source_count', 0)
            
            md_lines.extend([
                f"### âœ… {category}",
                f"- **æ–‡ç« æ•°é‡**: {article_count}",
                f"- **è®¢é˜…æºæ•°é‡**: {source_count}",
                f"- **æŠ¥å‘Šæ–‡ä»¶**: [{file_name}](./{file_name})",
                ""
            ])
        else:
            error_msg = result.get('error', 'æœªçŸ¥é”™è¯¯')
            md_lines.extend([
                f"### âŒ {category}",
                f"- **çŠ¶æ€**: ç”Ÿæˆå¤±è´¥",
                f"- **é”™è¯¯**: {error_msg}",
                ""
            ])
    
    # ä¿å­˜æ€»è§ˆæŠ¥å‘Š
    try:
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(md_lines))
        print(f"\nğŸ“‹ æ€»è§ˆæŠ¥å‘Šå·²ç”Ÿæˆ: {summary_file}")
    except Exception as e:
        print(f"\nâŒ æ€»è§ˆæŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

def generate_all_categories_news(rss_sources, hours_limit=24, output_dir="news"):
    """
    ä¸€é”®ç”Ÿæˆæ‰€æœ‰åˆ†ç±»çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š
    
    å‚æ•°:
        config_file_path (str): RSSé…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨æ ‡å‡†è·¯å¾„
        hours_limit (int): æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰ï¼Œé»˜è®¤24å°æ—¶
        output_dir (str): è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º"news"
        
    è¿”å›:
        dict: ç”Ÿæˆç»“æœ
    """
    try:
        if not rss_sources:
            print("âŒ æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•RSSè®¢é˜…æºé…ç½®")
            return {}
        
        print(f"ğŸ“š æˆåŠŸåŠ è½½ {len(rss_sources)} ä¸ªRSSè®¢é˜…æºé…ç½®")
        
        # æŒ‰åˆ†ç±»ç”Ÿæˆæ–°é—»æŠ¥å‘Š
        results = generate_news_by_categories(
            rss_sources=rss_sources,
            hours_limit=hours_limit,
            output_dir=output_dir
        )
        
        # è¾“å‡ºæœ€ç»ˆç»Ÿè®¡
        print("\n" + "="*60)
        print("ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡:")
        successful = sum(1 for r in results.values() if r.get('success', False))
        total_articles = sum(r.get('article_count', 0) for r in results.values())
        
        print(f"  âœ… æˆåŠŸç”Ÿæˆ: {successful}/{len(results)} ä¸ªåˆ†ç±»")
        print(f"  ğŸ“° æ–‡ç« æ€»æ•°: {total_articles}")
        print(f"  ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(output_dir)}")
        
        return results
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆæ–°é—»æŠ¥å‘Šæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return {}


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    try:
        # é€‰æ‹©è¿è¡Œæ¨¡å¼
        print("è¯·é€‰æ‹©è¿è¡Œæ¨¡å¼:")
        print("1. ç”Ÿæˆå•ä¸ªåˆ†ç±»çš„æ–°é—»æ±‡æ€» (å¦‚AIåˆ†ç±»)")
        print("2. ç”Ÿæˆæ‰€æœ‰åˆ†ç±»çš„æ–°é—»æ±‡æ€»")
        
        choice = input("è¯·è¾“å…¥é€‰æ‹© (1 æˆ– 2): ").strip()
        
        if choice == "1":
            # åŸæœ‰çš„å•åˆ†ç±»æ¨¡å¼
            from load_rss_url import load_rss_sources, get_rss_urls_by_category
            RSS_CONFIG_FILE = r'RSS feed URL\rss_feed_url.json'
            rss_sources = load_rss_sources(RSS_CONFIG_FILE)

            # ç­›é€‰AIç›¸å…³çš„RSSè®¢é˜…æº
            ai_rss_urls = get_rss_urls_by_category(rss_sources, category='AI')

            if not ai_rss_urls:
                print("æ²¡æœ‰æ‰¾åˆ°AIç›¸å…³çš„RSSè®¢é˜…æºã€‚")
            else:
                # ç”Ÿæˆæœ€è¿‘24å°æ—¶çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š
                output_file = f"news_summary_{datetime.now().strftime('%Y%m%d')}.md"
                markdown_report = get_recent_articles_summary(
                    rss_urls=ai_rss_urls,
                    hours_limit=24,  # å¯ä»¥ä¿®æ”¹æ—¶é—´é™åˆ¶
                    output_file=output_file,
                    rss_sources=rss_sources  # ä¼ é€’RSSæºä¿¡æ¯
                )
                
                print("\n" + "="*50)
                print("MarkdownæŠ¥å‘Šé¢„è§ˆ:")
                print("="*50)
                print(markdown_report[:500] + "..." if len(markdown_report) > 500 else markdown_report)
        
        elif choice == "2":
            # æ–°çš„å¤šåˆ†ç±»æ¨¡å¼
            print("\nğŸš€ å¼€å§‹ç”Ÿæˆæ‰€æœ‰åˆ†ç±»çš„æ–°é—»æ±‡æ€»æŠ¥å‘Š...")
            
            from load_rss_url import load_rss_sources
            # åŠ è½½RSSè®¢é˜…æºé…ç½®
            RSS_CONFIG_FILE = r'RSS feed URL\rss_feed_url.json'
            rss_sources = load_rss_sources(RSS_CONFIG_FILE)
            
            # å¯ä»¥è‡ªå®šä¹‰å‚æ•°
            hours_limit = 24  # æ—¶é—´é™åˆ¶
            timestamp = datetime.now().strftime('%Y%m%d')
            output_dir = f"news/{timestamp}"  # è¾“å‡ºç›®å½•

            results = generate_all_categories_news(
                rss_sources=rss_sources,
                hours_limit=hours_limit,
                output_dir=output_dir
            )
            
            if results:
                print(f"\nâœ¨ æ‰€æœ‰æŠ¥å‘Šå·²ç”Ÿæˆå®Œæˆï¼Œè¯·æŸ¥çœ‹ '{output_dir}' ç›®å½•")
            else:
                print("\nâŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥")
        
        else:
            print("æ— æ•ˆçš„é€‰æ‹©ï¼Œç¨‹åºé€€å‡ºã€‚")
                
    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œæ—¶å‡ºé”™: {e}")
    finally:
        gc.collect()
