"""
ç´¯ç§¯RSS Feedç”Ÿæˆå™¨ - ä¼˜åŒ–ç‰ˆ

è¯¥è„šæœ¬åŸºäºç´¯ç§¯æ–°é—»æ–‡æ¡£ç”ŸæˆRSS Feedï¼Œæ”¯æŒå¢é‡æ›´æ–°å’Œä¸¥æ ¼å»é‡
æ–°å¢åŠŸèƒ½ï¼š
1. åŸºäºå†å²è®°å½•çš„å»é‡æœºåˆ¶
2. æ—¶é—´çª—å£æ§åˆ¶ï¼Œé¿å…å‘å¸ƒè¿‡æ—§çš„æ–°é—»
3. å†…å®¹æŒ‡çº¹è¯†åˆ«ï¼Œé˜²æ­¢ç›¸ä¼¼å†…å®¹é‡å¤
4. å‘å¸ƒå†å²è·Ÿè¸ªï¼Œç¡®ä¿æ¯æ¬¡æ›´æ–°åªåŒ…å«æ–°å†…å®¹
"""

import os
import sys
import re
import json
import hashlib
from datetime import datetime, timedelta
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from urllib.parse import urlparse
import difflib

try:
    from src.ai_news_filter import NewsQualityFilter
    AI_FILTER_AVAILABLE = True
except ImportError:
    print("âš ï¸ AIç­›é€‰æ¨¡å—ä¸å¯ç”¨ï¼Œå°†è·³è¿‡AIç­›é€‰åŠŸèƒ½")
    AI_FILTER_AVAILABLE = False

def create_content_fingerprint(title, link, description=""):
    """
    åˆ›å»ºå†…å®¹æŒ‡çº¹ï¼Œç”¨äºç²¾ç¡®å»é‡
    
    å‚æ•°:
        title (str): æ–‡ç« æ ‡é¢˜
        link (str): æ–‡ç« é“¾æ¥
        description (str): æ–‡ç« æè¿°ï¼ˆå¯é€‰ï¼‰
        
    è¿”å›:
        str: å†…å®¹æŒ‡çº¹
    """
    # æ¸…ç†æ ‡é¢˜ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·ï¼Œè½¬æ¢ä¸ºå°å†™
    clean_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
    clean_title = ' '.join(clean_title.split())  # æ ‡å‡†åŒ–ç©ºæ ¼
    
    # æ¸…ç†é“¾æ¥ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µ
    parsed_url = urlparse(link)
    clean_link = f"{parsed_url.netloc}{parsed_url.path}".lower()
    
    # ç”Ÿæˆç»„åˆæŒ‡çº¹
    content = f"{clean_title}|{clean_link}|{description[:100]}"
    return hashlib.sha256(content.encode('utf-8')).hexdigest()

def calculate_title_similarity(title1, title2):
    """
    è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦
    
    å‚æ•°:
        title1 (str): æ ‡é¢˜1
        title2 (str): æ ‡é¢˜2
        
    è¿”å›:
        float: ç›¸ä¼¼åº¦ (0-1)
    """
    # æ¸…ç†å’Œæ ‡å‡†åŒ–æ ‡é¢˜
    def clean_title(title):
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[^\w\s]', ' ', title.lower())
        # æ ‡å‡†åŒ–ç©ºæ ¼
        return ' '.join(cleaned.split())
    
    cleaned_title1 = clean_title(title1)
    cleaned_title2 = clean_title(title2)
    
    # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
    similarity = difflib.SequenceMatcher(None, cleaned_title1, cleaned_title2).ratio()
    
    # åŒæ—¶æ£€æŸ¥è¯æ±‡é‡å åº¦
    words1 = set(cleaned_title1.split())
    words2 = set(cleaned_title2.split())
    
    if words1 and words2:
        word_overlap = len(words1.intersection(words2)) / len(words1.union(words2))
        # å–ä¸¤ç§æ–¹æ³•çš„æœ€å¤§å€¼
        similarity = max(similarity, word_overlap)
    
    return similarity

class RSSHistoryManager:
    """RSSå‘å¸ƒå†å²ç®¡ç†å™¨"""
    
    def __init__(self, history_file="rss_history.json"):
        self.history_file = history_file
        self.history_data = self.load_history()
    
    def load_history(self):
        """åŠ è½½å†å²è®°å½•"""
        if os.path.exists(self.history_file):
            try:
                with open(self.history_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                pass
        return {"published_articles": {}, "last_update": {}}
    
    def save_history(self):
        """ä¿å­˜å†å²è®°å½•"""
        try:
            with open(self.history_file, 'w', encoding='utf-8') as f:
                json.dump(self.history_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"  âš ï¸ ä¿å­˜å†å²è®°å½•å¤±è´¥: {e}")
    
    def is_article_published(self, category, fingerprint):
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦å·²å‘å¸ƒ"""
        category_articles = self.history_data["published_articles"].get(category, {})
        return fingerprint in category_articles
    
    def add_published_article(self, category, fingerprint, article_info):
        """æ·»åŠ å·²å‘å¸ƒçš„æ–‡ç« è®°å½•"""
        if category not in self.history_data["published_articles"]:
            self.history_data["published_articles"][category] = {}
        
        self.history_data["published_articles"][category][fingerprint] = {
            "title": article_info["title"],
            "link": article_info["link"],
            "published_date": article_info.get("pub_date", ""),
            "first_seen": datetime.now().isoformat()
        }
    
    def update_last_update_time(self, category):
        """æ›´æ–°æœ€åæ›´æ–°æ—¶é—´"""
        self.history_data["last_update"][category] = datetime.now().isoformat()
    
    def get_last_update_time(self, category):
        """è·å–æœ€åæ›´æ–°æ—¶é—´"""
        last_update_str = self.history_data["last_update"].get(category)
        if last_update_str:
            try:
                return datetime.fromisoformat(last_update_str)
            except:
                pass
        return None
    
    def cleanup_old_records(self, days=30):
        """æ¸…ç†è¿‡æ—§çš„è®°å½•"""
        cutoff_date = datetime.now() - timedelta(days=days)
        
        for category in list(self.history_data["published_articles"].keys()):
            articles = self.history_data["published_articles"][category]
            
            # æ¸…ç†è¿‡æ—§çš„æ–‡ç« è®°å½•
            articles_to_remove = []
            for fingerprint, article_info in articles.items():
                try:
                    first_seen = datetime.fromisoformat(article_info["first_seen"])
                    if first_seen < cutoff_date:
                        articles_to_remove.append(fingerprint)
                except:
                    # å¦‚æœè§£æå¤±è´¥ï¼Œä¿å®ˆèµ·è§ä¿ç•™è®°å½•
                    pass
            
            for fingerprint in articles_to_remove:
                del articles[fingerprint]
            
            print(f"  ğŸ§¹ åˆ†ç±» {category}: æ¸…ç†äº† {len(articles_to_remove)} æ¡è¿‡æœŸè®°å½•")

def advanced_deduplicate_articles(articles, category, history_manager, 
                                max_articles=50, similarity_threshold=0.85,
                                time_window_hours=72, only_new_articles=True):
    """
    é«˜çº§å»é‡åŠŸèƒ½ - å¢é‡æ›´æ–°æ¨¡å¼
    
    å‚æ•°:
        articles (list): æ–‡ç« åˆ—è¡¨
        category (str): åˆ†ç±»
        history_manager (RSSHistoryManager): å†å²ç®¡ç†å™¨
        max_articles (int): æœ€å¤§ä¿ç•™æ–‡ç« æ•°
        similarity_threshold (float): ç›¸ä¼¼åº¦é˜ˆå€¼
        time_window_hours (int): æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        only_new_articles (bool): æ˜¯å¦åªè¿”å›æ–°æ–‡ç« ï¼ˆå¢é‡æ›´æ–°æ¨¡å¼ï¼‰
        
    è¿”å›:
        list: å»é‡åçš„æ–‡ç« åˆ—è¡¨
    """
    if not articles:
        return []
    
    print(f"  ğŸ” å¼€å§‹å¢é‡å»é‡ï¼ŒåŸå§‹æ–‡ç« æ•°: {len(articles)}")
    print(f"  ğŸ“ ç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")
    print(f"  â° æ—¶é—´çª—å£: {time_window_hours} å°æ—¶")
    print(f"  ğŸ†• å¢é‡æ¨¡å¼: {only_new_articles}")
    
    # è·å–ä¸Šæ¬¡æ›´æ–°æ—¶é—´ - å…³é”®æ”¹åŠ¨
    last_update_time = history_manager.get_last_update_time(category)
    
    # æ—¶é—´æˆªæ­¢ç‚¹
    time_cutoff = datetime.now() - timedelta(hours=time_window_hours)
    
    # å¦‚æœæ˜¯å¢é‡æ›´æ–°æ¨¡å¼ä¸”æœ‰å†å²æ›´æ–°æ—¶é—´ï¼Œä½¿ç”¨æ›´ä¸¥æ ¼çš„æ—¶é—´ç­›é€‰
    if only_new_articles and last_update_time:
        # ä½¿ç”¨ä¸Šæ¬¡æ›´æ–°æ—¶é—´ä½œä¸ºæˆªæ­¢ç‚¹ï¼Œåªå–æ›´æ–°çš„æ–‡ç« 
        time_cutoff = max(time_cutoff, last_update_time)
        print(f"  ğŸ“… å¢é‡æ›´æ–°ï¼šåªè·å– {last_update_time.strftime('%Y-%m-%d %H:%M')} ä¹‹åçš„æ–‡ç« ")
    
    deduplicated_articles = []
    seen_fingerprints = set()
    removed_reasons = {
        "å·²å‘å¸ƒ": 0,
        "URLé‡å¤": 0,
        "æ ‡é¢˜ç›¸ä¼¼": 0,
        "è¿‡æ—§": 0,
        "è¶…å‡ºé™åˆ¶": 0,
        "æ—©äºä¸Šæ¬¡æ›´æ–°": 0  # æ–°å¢ç»Ÿè®¡é¡¹
    }
    
    for article in articles:
        title = article.get('title', '').strip()
        link = article.get('link', '').strip()
        pub_date_str = article.get('pub_date', '')
        
        if not title or not link:
            continue
        
        # 1. æ£€æŸ¥å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨çª—å£å†…ï¼ˆå¢å¼ºç‰ˆï¼‰
        article_too_old = False
        try:
            if pub_date_str and pub_date_str != 'æ—¶é—´æœªçŸ¥':
                # å°è¯•è§£æå‘å¸ƒæ—¶é—´
                for fmt in ['%Y-%m-%d %H:%M', '%a, %d %b %Y %H:%M:%S %Z', 
                           '%a, %d %b %Y %H:%M:%S GMT', '%Y-%m-%d %H:%M:%S']:
                    try:
                        pub_date = datetime.strptime(pub_date_str, fmt)
                        
                        # å¢é‡æ›´æ–°æ¨¡å¼ï¼šæ£€æŸ¥æ˜¯å¦æ—©äºä¸Šæ¬¡æ›´æ–°æ—¶é—´
                        if only_new_articles and last_update_time and pub_date <= last_update_time:
                            removed_reasons["æ—©äºä¸Šæ¬¡æ›´æ–°"] += 1
                            article_too_old = True
                            break
                        
                        # å¸¸è§„æ—¶é—´çª—å£æ£€æŸ¥
                        if pub_date < time_cutoff:
                            removed_reasons["è¿‡æ—§"] += 1
                            article_too_old = True
                            break
                        break
                    except ValueError:
                        continue
                else:
                    # å¦‚æœæ‰€æœ‰æ ¼å¼éƒ½å¤±è´¥ï¼Œåœ¨å¢é‡æ¨¡å¼ä¸‹è·³è¿‡
                    if only_new_articles and last_update_time:
                        removed_reasons["æ—©äºä¸Šæ¬¡æ›´æ–°"] += 1
                        article_too_old = True
        except:
            # è§£æå¤±è´¥ï¼Œåœ¨å¢é‡æ¨¡å¼ä¸‹ä¿å®ˆè·³è¿‡
            if only_new_articles and last_update_time:
                removed_reasons["æ—©äºä¸Šæ¬¡æ›´æ–°"] += 1
                article_too_old = True
        
        if article_too_old:
            continue
        
        # 2. ç”Ÿæˆå†…å®¹æŒ‡çº¹
        fingerprint = create_content_fingerprint(title, link, article.get('description', ''))
        
        # 3. æ£€æŸ¥æ˜¯å¦å·²åœ¨å†å²è®°å½•ä¸­å‘å¸ƒ
        if history_manager.is_article_published(category, fingerprint):
            removed_reasons["å·²å‘å¸ƒ"] += 1
            continue
        
        # 4. æ£€æŸ¥å½“å‰æ‰¹æ¬¡ä¸­çš„é‡å¤
        if fingerprint in seen_fingerprints:
            removed_reasons["URLé‡å¤"] += 1
            continue
        
        # 5. æ£€æŸ¥ä¸å½“å‰æ‰¹æ¬¡ä¸­å…¶ä»–æ–‡ç« çš„ç›¸ä¼¼æ€§
        is_similar = False
        for existing_article in deduplicated_articles:
            existing_title = existing_article.get('title', '')
            similarity = calculate_title_similarity(title, existing_title)
            
            if similarity > similarity_threshold:
                removed_reasons["æ ‡é¢˜ç›¸ä¼¼"] += 1
                print(f"    ğŸ”„ ç›¸ä¼¼æ ‡é¢˜ ({similarity:.2f}): {title[:30]}... â‰ˆ {existing_title[:30]}...")
                is_similar = True
                break
        
        if is_similar:
            continue
        
        # 6. æ£€æŸ¥æ˜¯å¦å·²è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°
        if len(deduplicated_articles) >= max_articles:
            removed_reasons["è¶…å‡ºé™åˆ¶"] += 1
            break
        
        # æ·»åŠ åˆ°ç»“æœåˆ—è¡¨
        seen_fingerprints.add(fingerprint)
        deduplicated_articles.append(article)
        
        # è®°å½•åˆ°å†å²ç®¡ç†å™¨ï¼ˆæ ‡è®°ä¸ºå·²å‘å¸ƒï¼‰
        history_manager.add_published_article(category, fingerprint, article)
    
    # è¾“å‡ºå»é‡ç»Ÿè®¡
    total_removed = sum(removed_reasons.values())
    print(f"  âœ… å¢é‡å»é‡å®Œæˆï¼Œç§»é™¤ {total_removed} ç¯‡æ–‡ç« ï¼Œä¿ç•™ {len(deduplicated_articles)} ç¯‡æ–°æ–‡ç« ")
    for reason, count in removed_reasons.items():
        if count > 0:
            print(f"    - {reason}: {count} ç¯‡")
    
    # ç‰¹åˆ«æç¤º
    if only_new_articles and len(deduplicated_articles) == 0:
        print(f"  â„¹ï¸ æ²¡æœ‰æ‰¾åˆ°æ–°æ–‡ç« ï¼ŒRSS Feedå°†ä¸ºç©º")
    elif only_new_articles:
        print(f"  ğŸ†• æœ¬æ¬¡æ›´æ–°å°†æ¨é€ {len(deduplicated_articles)} ç¯‡å…¨æ–°æ–‡ç« ")
    
    return deduplicated_articles

def parse_cumulative_markdown_optimized(md_file_path, category, history_manager, 
                                       max_recent_articles=50, time_window_hours=72,
                                       enable_ai_filter=True, ai_filter_count=10):
    """
    ä¼˜åŒ–ç‰ˆMarkdownè§£æï¼ŒåŒ…å«å¢é‡æ›´æ–°å’ŒAIç­›é€‰
    """
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {md_file_path}: {e}")
        return None
    
    info = {
        'title': '',
        'description': '',
        'pub_date': '',
        'articles': []
    }
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'^# (.+)', content, re.MULTILINE)
    if title_match:
        info['title'] = title_match.group(1).strip()
    
    # æå–æœ€åæ›´æ–°æ—¶é—´
    time_match = re.search(r'\*\*æœ€åæ›´æ–°æ—¶é—´\*\*:\s*(.+)', content)
    if time_match:
        time_str = time_match.group(1).strip()
        try:
            pub_date = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
            info['pub_date'] = pub_date.strftime('%a, %d %b %Y %H:%M:%S GMT')
        except ValueError:
            info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    else:
        info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    # è®¾ç½®æè¿°
    info['description'] = f"{category} åˆ†ç±»æœ€æ–°æ–°é—»ï¼Œå¢é‡æ›´æ–°ç¡®ä¿å†…å®¹æ–°é²œ"
    
    # æå–æ–‡ç« ï¼ˆæå–æ›´å¤šæ–‡ç« ä»¥ä¾¿å»é‡åä»æœ‰è¶³å¤Ÿæ•°é‡ï¼‰
    extraction_limit = max_recent_articles * 10  # å¢åŠ æå–å€æ•°ï¼Œç¡®ä¿æœ‰è¶³å¤Ÿçš„å€™é€‰æ–‡ç« 
    article_pattern = r'#### \[(.+?)\]\((.+?)\)\s*(?:\*\*å‘å¸ƒæ—¶é—´\*\*:\s*(.+?)(?:\n|$))?'
    articles = re.findall(article_pattern, content, re.MULTILINE | re.DOTALL)
    
    # é¦–å…ˆæå–æ›´å¤šæ–‡ç« 
    articles = articles[:extraction_limit]
    
    raw_articles = []
    for title, link, pub_time in articles:
        clean_title = title.replace('\\[', '[').replace('\\]', ']').strip()
        clean_link = link.strip()
        clean_pub_time = pub_time.strip() if pub_time else ''
        
        # è½¬æ¢å‘å¸ƒæ—¶é—´æ ¼å¼
        rss_pub_time = ''
        if clean_pub_time:
            try:
                for fmt in ['%Y-%m-%d %H:%M', '%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%d %H:%M:%S']:
                    try:
                        dt = datetime.strptime(clean_pub_time, fmt)
                        rss_pub_time = dt.strftime('%a, %d %b %Y %H:%M:%S GMT')
                        break
                    except ValueError:
                        continue
            except:
                pass
        
        if not rss_pub_time:
            rss_pub_time = info['pub_date']
        
        raw_articles.append({
            'title': clean_title,
            'link': clean_link,
            'pub_date': rss_pub_time,
            'description': clean_title
        })
    
    # è¿›è¡Œå¢é‡å»é‡å¤„ç†
    deduplicated_articles = advanced_deduplicate_articles(
        raw_articles, category, history_manager, 
        max_recent_articles, time_window_hours=time_window_hours,
        only_new_articles=True
    )
    
    # æ–°å¢ï¼šAIç­›é€‰æ­¥éª¤
    if enable_ai_filter and AI_FILTER_AVAILABLE and len(deduplicated_articles) > ai_filter_count:
        try:
            print(f"  ğŸ¤– å¯åŠ¨AIç­›é€‰åŠŸèƒ½...")
            ai_filter = NewsQualityFilter()
            filtered_articles = ai_filter.filter_articles(
                deduplicated_articles, category, ai_filter_count
            )
            info['articles'] = filtered_articles
            
            # æ›´æ–°å†å²è®°å½•ï¼ˆåªè®°å½•ç­›é€‰åçš„æ–‡ç« ï¼‰
            for article in filtered_articles:
                fingerprint = create_content_fingerprint(
                    article['title'], article['link'], article.get('description', '')
                )
                history_manager.add_published_article(category, fingerprint, article)
                
        except Exception as e:
            print(f"  âŒ AIç­›é€‰å¤±è´¥ï¼Œä½¿ç”¨å»é‡åçš„æ–‡ç« : {e}")
            info['articles'] = deduplicated_articles[:ai_filter_count]
    else:
        if not enable_ai_filter:
            print(f"  ğŸ”§ AIç­›é€‰å·²ç¦ç”¨")
        elif not AI_FILTER_AVAILABLE:
            print(f"  âš ï¸ AIç­›é€‰æ¨¡å—ä¸å¯ç”¨")
        else:
            print(f"  ğŸ“Š æ–‡ç« æ•°é‡ä¸è¶³ï¼Œè·³è¿‡AIç­›é€‰")
        info['articles'] = deduplicated_articles[:ai_filter_count]
    
    return info

def get_original_rss_filename(category):
    """æ ¹æ®åˆ†ç±»è·å–åŸæœ‰çš„RSSæ–‡ä»¶å"""
    category_filename_map = {
        'Finance': 'financefreenewsagent.xml',
        'finance': 'financefreenewsagent.xml',
        'Technology': 'technologyfreenewsagent.xml',
        'technology': 'technologyfreenewsagent.xml',
        'AI': 'aifreenewsagent.xml',
        'ai': 'aifreenewsagent.xml',
        'äººå·¥æ™ºèƒ½': 'aifreenewsagent.xml',
    }
    
    if category in category_filename_map:
        return category_filename_map[category]
    
    category_lower = category.lower()
    for key, filename in category_filename_map.items():
        if key.lower() == category_lower:
            return filename
    
    safe_category = category.lower().replace(' ', '').replace('_', '').replace('-', '')
    return f"{safe_category}freenewsagent.xml"

def read_existing_rss_metadata(xml_file_path):
    """è¯»å–ç°æœ‰RSSæ–‡ä»¶çš„å…ƒæ•°æ®"""
    metadata = {}
    
    if not os.path.exists(xml_file_path):
        return metadata
    
    try:
        with open(xml_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        follow_challenge_match = re.search(
            r'<follow_challenge>\s*<feedId>(\d+)</feedId>\s*<userId>([^<]+)</userId>\s*</follow_challenge>',
            content, re.MULTILINE | re.DOTALL
        )
        
        if follow_challenge_match:
            metadata['follow_challenge'] = {
                'feedId': follow_challenge_match.group(1),
                'userId': follow_challenge_match.group(2)
            }
    
    except Exception as e:
        print(f"  è¯»å–RSSå…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
    
    return metadata

def get_category_follow_challenge(category):
    """æ ¹æ®åˆ†ç±»è·å–è®¤è¯ä¿¡æ¯"""
    follow_challenges = {
        'ai': {
            'feedId': '150782771375663104',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        },
        'technology': {
            'feedId': '150782874584542208',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        },
        'finance': {
            'feedId': '150806458867930112',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        }
    }
    
    category_lower = category.lower()
    if category_lower in ['äººå·¥æ™ºèƒ½']:
        category_lower = 'ai'
    
    return follow_challenges.get(category_lower, None)

def generate_cumulative_rss_xml(news_info, category, base_url="https://zskksz.asia/News-Agent", 
                               existing_metadata=None):
    """ç”Ÿæˆç´¯ç§¯RSS XMLå†…å®¹"""
    rss = Element('rss')
    rss.set('version', '2.0')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    
    channel = SubElement(rss, 'channel')
    
    original_filename = get_original_rss_filename(category)
    
    title = SubElement(channel, 'title')
    title.text = f"{category} æ–°é—»æ±‡æ€» - Free News Agent"
    
    link = SubElement(channel, 'link')
    link.text = f"{base_url}/feed/{original_filename}"
    
    description = SubElement(channel, 'description')
    description.text = f"{category} åˆ†ç±»çš„æœ€æ–°æ–°é—»æ±‡æ€»ï¼Œç”± Free News Agent è‡ªåŠ¨ç”Ÿæˆï¼ŒGemini AI ç­›é€‰ä¼˜è´¨å†…å®¹ã€‚"
    
    language = SubElement(channel, 'language')
    language.text = "zh-CN"
    
    pub_date = SubElement(channel, 'pubDate')
    pub_date.text = news_info.get('pub_date', datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT'))
    
    last_build_date = SubElement(channel, 'lastBuildDate')
    last_build_date.text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    generator = SubElement(channel, 'generator')
    generator.text = "News Agent RSS Generator v2.0 (Optimized)"
    
    # æ·»åŠ è®¤è¯ä¿¡æ¯
    follow_challenge_info = None
    if existing_metadata and 'follow_challenge' in existing_metadata:
        follow_challenge_info = existing_metadata['follow_challenge']
    else:
        follow_challenge_info = get_category_follow_challenge(category)
    
    if follow_challenge_info:
        follow_challenge = SubElement(channel, 'follow_challenge')
        feed_id = SubElement(follow_challenge, 'feedId')
        feed_id.text = follow_challenge_info['feedId']
        user_id = SubElement(follow_challenge, 'userId')
        user_id.text = follow_challenge_info['userId']
    
    # æ·»åŠ atom:linkè‡ªå¼•ç”¨
    atom_link = SubElement(channel, 'atom:link')
    atom_link.set('href', f"{base_url}/feed/{original_filename}")
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æ·»åŠ æ–‡ç« é¡¹ç›®
    for article in news_info.get('articles', []):
        item = SubElement(channel, 'item')
        
        item_title = SubElement(item, 'title')
        item_title.text = article['title']
        
        item_link = SubElement(item, 'link')
        item_link.text = article['link']
        
        item_description = SubElement(item, 'description')
        item_description.text = f"<![CDATA[{article['description']}]]>"
        
        item_pub_date = SubElement(item, 'pubDate')
        item_pub_date.text = article['pub_date']
        
        guid = SubElement(item, 'guid')
        guid.set('isPermaLink', 'true')
        guid.text = article['link']
        
        item_category = SubElement(item, 'category')
        item_category.text = category
    
    # æ ¼å¼åŒ–XML
    rough_string = tostring(rss, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ", encoding='utf-8').decode('utf-8')

def get_cumulative_news_files(news_dir="cumulative_news"):
    """è·å–ç´¯ç§¯æ–°é—»æ–‡ä»¶åˆ—è¡¨"""
    if not os.path.exists(news_dir):
        print(f"âŒ ç´¯ç§¯æ–°é—»ç›®å½•ä¸å­˜åœ¨: {news_dir}")
        return []
    
    news_files = []
    for filename in os.listdir(news_dir):
        if filename.endswith('_cumulative.md'):
            news_files.append(os.path.join(news_dir, filename))
    
    return news_files

def extract_category_from_cumulative_filename(filename):
    """ä»ç´¯ç§¯æ–‡ä»¶åä¸­æå–åˆ†ç±»åç§°"""
    basename = os.path.basename(filename)
    category = basename.replace('_cumulative.md', '')
    return category

def main():
    """ä¸»å‡½æ•°ï¼šç”Ÿæˆå¢é‡RSS Feed"""
    print("=" * 60)
    print("ğŸ“¡ ç´¯ç§¯RSS Feedç”Ÿæˆå™¨ - AIç­›é€‰ç‰ˆ v1.0")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    news_dir = "cumulative_news"
    feed_dir = "feed"
    base_url = "https://zskksz.asia/News-Agent"
    max_articles_per_feed = 50  # å¢åŠ å€™é€‰æ–‡ç« æ•°
    ai_filter_count = 10  # AIç­›é€‰åçš„ç›®æ ‡æ•°é‡
    time_window_hours = 48
    similarity_threshold = 0.85
    enable_ai_filter = True  # æ˜¯å¦å¯ç”¨AIç­›é€‰
    
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  - ç´¯ç§¯æ–°é—»ç›®å½•: {news_dir}")
    print(f"  - Feedè¾“å‡ºç›®å½•: {feed_dir}")
    print(f"  - åŸºç¡€URL: {base_url}")
    print(f"  - å€™é€‰æ–‡ç« æ•°: {max_articles_per_feed}")
    print(f"  - AIç­›é€‰æ•°é‡: {ai_filter_count}")
    print(f"  - æ—¶é—´çª—å£: {time_window_hours} å°æ—¶")
    print(f"  - AIç­›é€‰: {'å¯ç”¨' if enable_ai_filter else 'ç¦ç”¨'}")
    print(f"  - æ¨¡å¼: å¢é‡æ›´æ–° + AIç­›é€‰ä¼˜è´¨æ–°é—»")
    print()
    
    # åˆå§‹åŒ–å†å²ç®¡ç†å™¨
    history_manager = RSSHistoryManager()
    print("ğŸ“š åˆå§‹åŒ–RSSå†å²ç®¡ç†å™¨...")
    
    # æ¸…ç†è¿‡æœŸè®°å½•
    history_manager.cleanup_old_records(days=30)
    
    # åˆ›å»ºfeedç›®å½•
    if not os.path.exists(feed_dir):
        os.makedirs(feed_dir)
        print(f"âœ… åˆ›å»ºFeedç›®å½•: {feed_dir}")
    
    # è·å–ç´¯ç§¯æ–°é—»æ–‡ä»¶
    news_files = get_cumulative_news_files(news_dir)
    
    if not news_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç´¯ç§¯æ–°é—»æ–‡ä»¶")
        return False
    
    print(f"ğŸ“° æ‰¾åˆ° {len(news_files)} ä¸ªç´¯ç§¯æ–°é—»æ–‡ä»¶")
    print("-" * 60)
    
    success_count = 0
    failed_count = 0
    total_new_articles = 0
    categories_with_updates = []
    categories_no_updates = []
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»ç”ŸæˆRSS Feed
    for news_file in news_files:
        category = extract_category_from_cumulative_filename(news_file)
        print(f"ğŸ”„ å¤„ç†åˆ†ç±»: {category}")
        
        # æ˜¾ç¤ºä¸Šæ¬¡æ›´æ–°æ—¶é—´
        last_update = history_manager.get_last_update_time(category)
        if last_update:
            print(f"  ğŸ“… ä¸Šæ¬¡æ›´æ–°: {last_update.strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            print(f"  ğŸ“… é¦–æ¬¡è¿è¡Œï¼Œå°†è·å–æ‰€æœ‰æ–‡ç« ")
        
        try:
            # ä½¿ç”¨å¢é‡æ›´æ–°+AIç­›é€‰è§£æ
            news_info = parse_cumulative_markdown_optimized(
                news_file, category, history_manager, 
                max_articles_per_feed, time_window_hours,
                enable_ai_filter, ai_filter_count
            )
            
            if not news_info:
                print(f"  âŒ è§£æå¤±è´¥")
                failed_count += 1
                continue
            
            article_count = len(news_info.get('articles', []))
            total_new_articles += article_count
            
            if article_count == 0:
                print(f"  ğŸ“„ æ²¡æœ‰æ–°æ–‡ç« ï¼Œè·³è¿‡RSSç”Ÿæˆ")
                categories_no_updates.append(category)
                success_count += 1  # ä»ç„¶ç®—ä½œæˆåŠŸï¼Œåªæ˜¯æ²¡æœ‰æ›´æ–°
                
                # æ›´æ–°å†å²è®°å½•æ—¶é—´ï¼ˆå³ä½¿æ²¡æœ‰æ–°æ–‡ç« ï¼‰
                history_manager.update_last_update_time(category)
                continue
            
            print(f"  ğŸ“„ åŒ…å« {article_count} ç¯‡æ–°æ–‡ç« ")
            categories_with_updates.append((category, article_count))
            
            # è·å–åŸæœ‰RSSæ–‡ä»¶å
            original_filename = get_original_rss_filename(category)
            xml_path = os.path.join(feed_dir, original_filename)
            
            # è¯»å–ç°æœ‰RSSæ–‡ä»¶çš„å…ƒæ•°æ®
            existing_metadata = read_existing_rss_metadata(xml_path)
            if existing_metadata and 'follow_challenge' in existing_metadata:
                print(f"  ğŸ“‹ ä¿ç•™ç°æœ‰è®¤è¯ä¿¡æ¯: feedId={existing_metadata['follow_challenge']['feedId']}")
            else:
                default_auth = get_category_follow_challenge(category)
                if default_auth:
                    print(f"  ğŸ” ä½¿ç”¨é»˜è®¤è®¤è¯ä¿¡æ¯: feedId={default_auth['feedId']}")
                else:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°è®¤è¯ä¿¡æ¯")
            
            # ç”ŸæˆRSS XML
            rss_xml = generate_cumulative_rss_xml(news_info, category, base_url, existing_metadata)
            
            # ç›´æ¥è¦†ç›–åŸæœ‰XMLæ–‡ä»¶
            with open(xml_path, 'w', encoding='utf-8') as f:
                f.write(rss_xml)
            
            # æ›´æ–°å†å²è®°å½•
            history_manager.update_last_update_time(category)
            
            print(f"  âœ… æ›´æ–°æˆåŠŸ: {original_filename}")
            success_count += 1
            
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            failed_count += 1
    
    # ä¿å­˜å†å²è®°å½•
    history_manager.save_history()
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š å¢é‡æ›´æ–°ç»“æœç»Ÿè®¡:")
    print("=" * 60)
    print(f"  âœ… æˆåŠŸå¤„ç†: {success_count} ä¸ªåˆ†ç±»")
    print(f"  âŒ å¤„ç†å¤±è´¥: {failed_count} ä¸ªåˆ†ç±»")
    print(f"  ğŸ†• æ–°æ–‡ç« æ€»æ•°: {total_new_articles}")
    print(f"   è¾“å‡ºç›®å½•: {os.path.abspath(feed_dir)}")
    
    if categories_with_updates:
        print(f"\nğŸ“¡ æœ‰æ›´æ–°çš„åˆ†ç±» ({len(categories_with_updates)} ä¸ª):")
        for category, count in categories_with_updates:
            original_filename = get_original_rss_filename(category)
            print(f"  - {category}: {count} ç¯‡æ–°æ–‡ç«  â†’ {base_url}/feed/{original_filename}")
    
    if categories_no_updates:
        print(f"\nğŸ’¤ æ— æ›´æ–°çš„åˆ†ç±» ({len(categories_no_updates)} ä¸ª):")
        for category in categories_no_updates:
            print(f"  - {category}: æ²¡æœ‰æ–°æ–‡ç« ")
    
    if total_new_articles > 0:
        print(f"\nğŸ‰ å¢é‡æ›´æ–°å®Œæˆï¼ç”¨æˆ·å°†åªæ”¶åˆ° {total_new_articles} ç¯‡å…¨æ–°æ–‡ç« ï¼")
    else:
        print(f"\nğŸ˜´ æœ¬æ¬¡è¿è¡Œæ²¡æœ‰æ–°æ–‡ç« ï¼ŒRSS Feedä¿æŒä¸å˜")
    
    return success_count > 0

if __name__ == "__main__":
    success = main()
    
    if success:
        print("\nğŸ¯ ä»»åŠ¡å®Œæˆï¼")
    else:
        print("\nğŸ’¥ ä»»åŠ¡å¤±è´¥ï¼")
        sys.exit(1)