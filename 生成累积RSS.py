"""
ç´¯ç§¯RSS Feedç”Ÿæˆå™¨

è¯¥è„šæœ¬åŸºäºç´¯ç§¯æ–°é—»æ–‡æ¡£ç”ŸæˆRSS Feedï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼Œç›´æ¥è¦†ç›–åŸæœ‰æ–‡ä»¶å¹¶ä¿ç•™è®¤è¯ä¿¡æ¯
"""

import os
import sys
import re
from datetime import datetime
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom

def parse_cumulative_markdown(md_file_path, max_recent_articles=20):
    """
    è§£æç´¯ç§¯Markdownæ–‡ä»¶ï¼Œæå–æœ€è¿‘çš„æ–‡ç« ä¿¡æ¯
    
    å‚æ•°:
        md_file_path (str): ç´¯ç§¯Markdownæ–‡ä»¶è·¯å¾„
        max_recent_articles (int): RSSä¸­åŒ…å«çš„æœ€å¤§æ–‡ç« æ•°
        
    è¿”å›:
        dict: åŒ…å«æ–°é—»ä¿¡æ¯çš„å­—å…¸
    """
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {md_file_path}: {e}")
        return None
    
    info = {
        'title': '',
        'description': '',
        'pub_date': '',
        'articles': []
    }
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'^# (.+)', content, re.MULTILINE)
    if title_match:
        info['title'] = title_match.group(1).strip()
    
    # æå–æœ€åæ›´æ–°æ—¶é—´
    time_match = re.search(r'\*\*æœ€åæ›´æ–°æ—¶é—´\*\*:\s*(.+)', content)
    if time_match:
        time_str = time_match.group(1).strip()
        try:
            pub_date = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
            info['pub_date'] = pub_date.strftime('%a, %d %b %Y %H:%M:%S GMT')
        except ValueError:
            info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    else:
        info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    # è®¾ç½®æè¿°
    info['description'] = "ç´¯ç§¯æ–°é—»æ±‡æ€»ï¼ŒæŒç»­æ›´æ–°çš„ç§‘æŠ€èµ„è®¯"
    
    # æå–æ–‡ç« ï¼ˆä¼˜å…ˆæå–æœ€æ–°çš„æ–‡ç« ï¼‰
    article_pattern = r'#### \[(.+?)\]\((.+?)\)\s*(?:\*\*å‘å¸ƒæ—¶é—´\*\*:\s*(.+?)(?:\n|$))?'
    articles = re.findall(article_pattern, content, re.MULTILINE | re.DOTALL)
    
    # é™åˆ¶æ–‡ç« æ•°é‡ï¼Œåªå–æœ€æ–°çš„æ–‡ç« 
    articles = articles[:max_recent_articles]
    
    for title, link, pub_time in articles:
        clean_title = title.replace('\\[', '[').replace('\\]', ']').strip()
        clean_link = link.strip()
        clean_pub_time = pub_time.strip() if pub_time else ''
        
        # è½¬æ¢å‘å¸ƒæ—¶é—´æ ¼å¼
        rss_pub_time = ''
        if clean_pub_time:
            try:
                for fmt in ['%Y-%m-%d %H:%M', '%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%d %H:%M:%S']:
                    try:
                        dt = datetime.strptime(clean_pub_time, fmt)
                        rss_pub_time = dt.strftime('%a, %d %b %Y %H:%M:%S GMT')
                        break
                    except ValueError:
                        continue
            except:
                pass
        
        if not rss_pub_time:
            rss_pub_time = info['pub_date']
        
        info['articles'].append({
            'title': clean_title,
            'link': clean_link,
            'pub_date': rss_pub_time,
            'description': clean_title
        })
    
    return info

def get_original_rss_filename(category):
    """
    æ ¹æ®åˆ†ç±»è·å–åŸæœ‰çš„RSSæ–‡ä»¶å
    
    å‚æ•°:
        category (str): åˆ†ç±»åç§°
        
    è¿”å›:
        str: åŸæœ‰çš„RSSæ–‡ä»¶å
    """
    # å®šä¹‰åˆ†ç±»åˆ°åŸæœ‰æ–‡ä»¶åçš„æ˜ å°„
    category_filename_map = {
        'Finance': 'financefreenewsagent.xml',
        'finance': 'financefreenewsagent.xml',
        'Technology': 'technologyfreenewsagent.xml',
        'technology': 'technologyfreenewsagent.xml',
        'AI': 'aifreenewsagent.xml',
        'ai': 'aifreenewsagent.xml',
        'äººå·¥æ™ºèƒ½': 'aifreenewsagent.xml',
    }
    
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    if category in category_filename_map:
        return category_filename_map[category]
    
    # ç„¶åå°è¯•å°å†™åŒ¹é…
    category_lower = category.lower()
    for key, filename in category_filename_map.items():
        if key.lower() == category_lower:
            return filename
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
    safe_category = category.lower().replace(' ', '').replace('_', '').replace('-', '')
    return f"{safe_category}freenewsagent.xml"

def read_existing_rss_metadata(xml_file_path):
    """
    è¯»å–ç°æœ‰RSSæ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆå¦‚follow_challengeç­‰ï¼‰
    
    å‚æ•°:
        xml_file_path (str): RSSæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        dict: å…ƒæ•°æ®å­—å…¸
    """
    metadata = {}
    
    if not os.path.exists(xml_file_path):
        return metadata
    
    try:
        with open(xml_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–follow_challengeä¿¡æ¯
        follow_challenge_match = re.search(
            r'<follow_challenge>\s*<feedId>(\d+)</feedId>\s*<userId>([^<]+)</userId>\s*</follow_challenge>',
            content, re.MULTILINE | re.DOTALL
        )
        
        if follow_challenge_match:
            metadata['follow_challenge'] = {
                'feedId': follow_challenge_match.group(1),
                'userId': follow_challenge_match.group(2)
            }
    
    except Exception as e:
        print(f"  è¯»å–RSSå…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
    
    return metadata

def get_category_follow_challenge(category):
    """
    æ ¹æ®åˆ†ç±»è·å–è®¤è¯ä¿¡æ¯
    
    å‚æ•°:
        category (str): åˆ†ç±»åç§°
        
    è¿”å›:
        dict: è®¤è¯ä¿¡æ¯å­—å…¸
    """
    # å®šä¹‰åˆ†ç±»å¯¹åº”çš„è®¤è¯ä¿¡æ¯
    follow_challenges = {
        'ai': {
            'feedId': '150741279739242496',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        },
        'technology': {
            'feedId': '150742893699033088',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        },
        'finance': {
            'feedId': '150742893699033088',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        }
    }
    
    category_lower = category.lower()
    
    # æ˜ å°„ä¸­æ–‡åç§°
    if category_lower in ['äººå·¥æ™ºèƒ½']:
        category_lower = 'ai'
    
    return follow_challenges.get(category_lower, None)

def generate_cumulative_rss_xml(news_info, category, base_url="https://zskfree.github.io/News-Agent", 
                               existing_metadata=None):
    """
    ç”Ÿæˆç´¯ç§¯RSS XMLå†…å®¹ï¼Œä¿æŒåŸæœ‰çš„å…ƒæ•°æ®å’Œè®¤è¯ä¿¡æ¯
    
    å‚æ•°:
        news_info (dict): æ–°é—»ä¿¡æ¯
        category (str): åˆ†ç±»åç§°
        base_url (str): åŸºç¡€URL
        existing_metadata (dict): ç°æœ‰çš„å…ƒæ•°æ®
        
    è¿”å›:
        str: RSS XMLå­—ç¬¦ä¸²
    """
    rss = Element('rss')
    rss.set('version', '2.0')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    
    channel = SubElement(rss, 'channel')
    
    # æ ¹æ®åˆ†ç±»è·å–åŸæœ‰æ–‡ä»¶å
    original_filename = get_original_rss_filename(category)
    
    title = SubElement(channel, 'title')
    title.text = f"{category} æ–°é—»æ±‡æ€» - Free News Agent"
    
    link = SubElement(channel, 'link')
    link.text = f"{base_url}/feed/{original_filename}"
    
    description = SubElement(channel, 'description')
    description.text = f"{category} åˆ†ç±»çš„æœ€æ–°æ–°é—»æ±‡æ€»ï¼Œç”± Free News Agent è‡ªåŠ¨ç”Ÿæˆ"
    
    language = SubElement(channel, 'language')
    language.text = "zh-CN"
    
    pub_date = SubElement(channel, 'pubDate')
    pub_date.text = news_info.get('pub_date', datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT'))
    
    last_build_date = SubElement(channel, 'lastBuildDate')
    last_build_date.text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    generator = SubElement(channel, 'generator')
    generator.text = "News Agent RSS Generator"
    
    # æ·»åŠ è®¤è¯ä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨ç°æœ‰çš„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„
    follow_challenge_info = None
    if existing_metadata and 'follow_challenge' in existing_metadata:
        follow_challenge_info = existing_metadata['follow_challenge']
    else:
        follow_challenge_info = get_category_follow_challenge(category)
    
    if follow_challenge_info:
        follow_challenge = SubElement(channel, 'follow_challenge')
        feed_id = SubElement(follow_challenge, 'feedId')
        feed_id.text = follow_challenge_info['feedId']
        user_id = SubElement(follow_challenge, 'userId')
        user_id.text = follow_challenge_info['userId']
    
    # æ·»åŠ atom:linkè‡ªå¼•ç”¨
    atom_link = SubElement(channel, 'atom:link')
    atom_link.set('href', f"{base_url}/feed/{original_filename}")
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æ·»åŠ æ–‡ç« é¡¹ç›®
    for article in news_info.get('articles', []):
        item = SubElement(channel, 'item')
        
        item_title = SubElement(item, 'title')
        item_title.text = article['title']
        
        item_link = SubElement(item, 'link')
        item_link.text = article['link']
        
        item_description = SubElement(item, 'description')
        item_description.text = f"<![CDATA[{article['description']}]]>"
        
        item_pub_date = SubElement(item, 'pubDate')
        item_pub_date.text = article['pub_date']
        
        guid = SubElement(item, 'guid')
        guid.set('isPermaLink', 'true')
        guid.text = article['link']
        
        item_category = SubElement(item, 'category')
        item_category.text = category
    
    # æ ¼å¼åŒ–XML
    rough_string = tostring(rss, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ", encoding='utf-8').decode('utf-8')

def get_cumulative_news_files(news_dir="cumulative_news"):
    """
    è·å–ç´¯ç§¯æ–°é—»æ–‡ä»¶åˆ—è¡¨
    """
    if not os.path.exists(news_dir):
        print(f"âŒ ç´¯ç§¯æ–°é—»ç›®å½•ä¸å­˜åœ¨: {news_dir}")
        return []
    
    news_files = []
    for filename in os.listdir(news_dir):
        if filename.endswith('_cumulative.md'):
            news_files.append(os.path.join(news_dir, filename))
    
    return news_files

def extract_category_from_cumulative_filename(filename):
    """
    ä»ç´¯ç§¯æ–‡ä»¶åä¸­æå–åˆ†ç±»åç§°
    """
    basename = os.path.basename(filename)
    category = basename.replace('_cumulative.md', '')
    return category

def main():
    """
    ä¸»å‡½æ•°ï¼šç”Ÿæˆç´¯ç§¯RSS Feedï¼Œç›´æ¥è¦†ç›–åŸæœ‰æ–‡ä»¶å¹¶ä¿ç•™è®¤è¯ä¿¡æ¯
    """
    print("=" * 60)
    print("ğŸ“¡ ç´¯ç§¯RSS Feedç”Ÿæˆå™¨ (è¦†ç›–åŸæœ‰æ–‡ä»¶ï¼Œä¿ç•™è®¤è¯)")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    news_dir = "cumulative_news"
    feed_dir = "feed"
    base_url = "https://zskfree.github.io/News-Agent"
    max_articles_per_feed = 100  # æ¯ä¸ªRSS Feedæœ€å¤šåŒ…å«çš„æ–‡ç« æ•°
    
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  - ç´¯ç§¯æ–°é—»ç›®å½•: {news_dir}")
    print(f"  - Feedè¾“å‡ºç›®å½•: {feed_dir}")
    print(f"  - åŸºç¡€URL: {base_url}")
    print(f"  - æ¯Feedæœ€å¤§æ–‡ç« æ•°: {max_articles_per_feed}")
    print(f"  - æ¨¡å¼: è¦†ç›–åŸæœ‰RSSæ–‡ä»¶å¹¶ä¿ç•™è®¤è¯ä¿¡æ¯")
    print()
    
    # åˆ›å»ºfeedç›®å½•
    if not os.path.exists(feed_dir):
        os.makedirs(feed_dir)
        print(f"âœ… åˆ›å»ºFeedç›®å½•: {feed_dir}")
    
    # è·å–ç´¯ç§¯æ–°é—»æ–‡ä»¶
    news_files = get_cumulative_news_files(news_dir)
    
    if not news_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç´¯ç§¯æ–°é—»æ–‡ä»¶")
        return False
    
    print(f"ğŸ“° æ‰¾åˆ° {len(news_files)} ä¸ªç´¯ç§¯æ–°é—»æ–‡ä»¶")
    print("-" * 60)
    
    success_count = 0
    failed_count = 0
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»ç”ŸæˆRSS Feed
    for news_file in news_files:
        category = extract_category_from_cumulative_filename(news_file)
        print(f"ğŸ”„ å¤„ç†åˆ†ç±»: {category}")
        
        try:
            # è§£æMarkdownæ–‡ä»¶
            news_info = parse_cumulative_markdown(news_file, max_articles_per_feed)
            
            if not news_info:
                print(f"  âŒ è§£æå¤±è´¥")
                failed_count += 1
                continue
            
            article_count = len(news_info.get('articles', []))
            print(f"  ğŸ“„ è§£æåˆ° {article_count} ç¯‡æ–‡ç« ")
            
            # è·å–åŸæœ‰RSSæ–‡ä»¶å
            original_filename = get_original_rss_filename(category)
            xml_path = os.path.join(feed_dir, original_filename)
            
            # è¯»å–ç°æœ‰RSSæ–‡ä»¶çš„å…ƒæ•°æ®
            existing_metadata = read_existing_rss_metadata(xml_path)
            if existing_metadata and 'follow_challenge' in existing_metadata:
                print(f"  ğŸ“‹ ä¿ç•™ç°æœ‰è®¤è¯ä¿¡æ¯: feedId={existing_metadata['follow_challenge']['feedId']}")
            else:
                default_auth = get_category_follow_challenge(category)
                if default_auth:
                    print(f"  ğŸ” ä½¿ç”¨é»˜è®¤è®¤è¯ä¿¡æ¯: feedId={default_auth['feedId']}")
                else:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°è®¤è¯ä¿¡æ¯")
            
            # ç”ŸæˆRSS XML
            rss_xml = generate_cumulative_rss_xml(news_info, category, base_url, existing_metadata)
            
            # ç›´æ¥è¦†ç›–åŸæœ‰XMLæ–‡ä»¶
            with open(xml_path, 'w', encoding='utf-8') as f:
                f.write(rss_xml)
            
            print(f"  âœ… è¦†ç›–æ›´æ–°æˆåŠŸ: {original_filename}")
            success_count += 1
            
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
            failed_count += 1
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆç»“æœç»Ÿè®¡:")
    print("=" * 60)
    print(f"  âœ… æˆåŠŸæ›´æ–°: {success_count} ä¸ªRSS Feed")
    print(f"  âŒ æ›´æ–°å¤±è´¥: {failed_count} ä¸ª")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(feed_dir)}")
    print(f"  ğŸ”„ æ¨¡å¼: ç›´æ¥è¦†ç›–åŸæœ‰æ–‡ä»¶ï¼Œä¿ç•™è®¤è¯ä¿¡æ¯")
    
    if success_count > 0:
        print(f"\nğŸ“¡ æ›´æ–°çš„RSSè®¢é˜…åœ°å€ (æ— éœ€é‡æ–°è®¢é˜…):")
        for news_file in news_files:
            category = extract_category_from_cumulative_filename(news_file)
            original_filename = get_original_rss_filename(category)
            if os.path.exists(os.path.join(feed_dir, original_filename)):
                print(f"  - {category}: {base_url}/feed/{original_filename}")
    
    print(f"\nğŸ‰ RSS Feedæ›´æ–°å®Œæˆï¼ç”¨æˆ·æ— éœ€é‡æ–°è®¢é˜…ï¼")
    return success_count > 0

def show_help():
    """
    æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    """
    print("=" * 60)
    print("ğŸ“– ç´¯ç§¯RSS Feedç”Ÿæˆå™¨")
    print("=" * 60)
    print()
    print("åŠŸèƒ½è¯´æ˜:")
    print("  - åŸºäºç´¯ç§¯æ–°é—»æ–‡æ¡£ç”ŸæˆRSS Feed")
    print("  - ç›´æ¥è¦†ç›–åŸæœ‰çš„RSSæ–‡ä»¶ï¼Œä¿æŒURLä¸å˜")
    print("  - ä¿ç•™åŸæœ‰çš„è®¤è¯ä¿¡æ¯ï¼ˆfollow_challengeï¼‰")
    print("  - æ”¯æŒä»ç°æœ‰æ–‡ä»¶è¯»å–æˆ–ä½¿ç”¨é»˜è®¤è®¤è¯ä¿¡æ¯")
    print("  - ç”¨æˆ·æ— éœ€é‡æ–°è®¢é˜…RSSæº")
    print()
    print("æ–‡ä»¶æ˜ å°„:")
    print("  - Finance -> financefreenewsagent.xml")
    print("  - Technology -> technologyfreenewsagent.xml") 
    print("  - AI -> aifreenewsagent.xml")
    print("  - å…¶ä»–åˆ†ç±» -> [åˆ†ç±»å]freenewsagent.xml")
    print()
    print("è®¤è¯ä¿¡æ¯:")
    print("  - ä¼˜å…ˆä½¿ç”¨ç°æœ‰RSSæ–‡ä»¶ä¸­çš„è®¤è¯ä¿¡æ¯")
    print("  - å¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é¢„å®šä¹‰çš„é»˜è®¤è®¤è¯ä¿¡æ¯")
    print("  - ç¡®ä¿RSS Feedçš„æ‰€æœ‰æƒéªŒè¯æ­£å¸¸")
    print()
    print("è¾“å…¥æ–‡ä»¶:")
    print("  - cumulative_news/åˆ†ç±»å_cumulative.md")
    print()
    print("è¾“å‡ºæ–‡ä»¶:")
    print("  - feed/åŸæœ‰æ–‡ä»¶å.xmlï¼ˆç›´æ¥è¦†ç›–ï¼‰")
    print()
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  python ç”Ÿæˆç´¯ç§¯RSS.py")
    print("  python ç”Ÿæˆç´¯ç§¯RSS.py --help")

if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] in ['--help', '-h', 'help']:
        show_help()
    else:
        # æ‰§è¡Œä¸»ç¨‹åº
        success = main()
        
        if success:
            print("\nğŸ¯ ä»»åŠ¡å®Œæˆï¼")
        else:
            print("\nğŸ’¥ ä»»åŠ¡å¤±è´¥ï¼")
            sys.exit(1)