"""
ç´¯ç§¯RSS Feedç”Ÿæˆå™¨

è¯¥è„šæœ¬åŸºäºç´¯ç§¯æ–°é—»æ–‡æ¡£ç”ŸæˆRSS Feedï¼Œæ”¯æŒå¢é‡æ›´æ–°ï¼Œç›´æ¥è¦†ç›–åŸæœ‰æ–‡ä»¶å¹¶ä¿ç•™è®¤è¯ä¿¡æ¯
"""

import os
import sys
import re
from datetime import datetime
from xml.etree.ElementTree import Element, SubElement, tostring
from xml.dom import minidom
from urllib.parse import urlparse, parse_qs
import hashlib

def normalize_url(url):
    """
    æ ‡å‡†åŒ–URLï¼Œå»é™¤æŸ¥è¯¢å‚æ•°å’Œé”šç‚¹ï¼Œç”¨äºå»é‡æ¯”è¾ƒ
    
    å‚æ•°:
        url (str): åŸå§‹URL
        
    è¿”å›:
        str: æ ‡å‡†åŒ–åçš„URL
    """
    try:
        parsed = urlparse(url)
        # ç§»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µ
        normalized = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
        return normalized.lower().strip()
    except:
        return url.lower().strip()

def calculate_title_similarity_hash(title):
    """
    è®¡ç®—æ ‡é¢˜çš„ç›¸ä¼¼æ€§å“ˆå¸Œï¼Œç”¨äºæ£€æµ‹ç›¸ä¼¼æ ‡é¢˜
    
    å‚æ•°:
        title (str): æ–°é—»æ ‡é¢˜
        
    è¿”å›:
        str: æ ‡é¢˜çš„å“ˆå¸Œå€¼
    """
    # æ¸…ç†æ ‡é¢˜ï¼šç§»é™¤æ ‡ç‚¹ç¬¦å·ã€ç©ºæ ¼ï¼Œè½¬æ¢ä¸ºå°å†™
    import string
    cleaned_title = title.lower()
    # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
    cleaned_title = ''.join(char for char in cleaned_title if char not in string.punctuation)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    cleaned_title = ' '.join(cleaned_title.split())
    
    # è®¡ç®—MD5å“ˆå¸Œ
    return hashlib.md5(cleaned_title.encode('utf-8')).hexdigest()

def deduplicate_articles(articles, max_articles=50):
    """
    å»é™¤é‡å¤çš„æ–°é—»æ–‡ç« 
    
    å‚æ•°:
        articles (list): æ–‡ç« åˆ—è¡¨
        max_articles (int): æœ€å¤§ä¿ç•™æ–‡ç« æ•°
        
    è¿”å›:
        list: å»é‡åçš„æ–‡ç« åˆ—è¡¨
    """
    if not articles:
        return []
    
    print(f"  ğŸ” å¼€å§‹å»é‡ï¼ŒåŸå§‹æ–‡ç« æ•°: {len(articles)}")
    
    seen_urls = set()
    seen_title_hashes = set()
    deduplicated_articles = []
    
    for article in articles:
        title = article.get('title', '').strip()
        link = article.get('link', '').strip()
        
        if not title or not link:
            continue
        
        # æ ‡å‡†åŒ–URLè¿›è¡Œæ¯”è¾ƒ
        normalized_url = normalize_url(link)
        
        # è®¡ç®—æ ‡é¢˜çš„ç›¸ä¼¼æ€§å“ˆå¸Œ
        title_hash = calculate_title_similarity_hash(title)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºé‡å¤
        is_duplicate = False
        
        # 1. æ£€æŸ¥URLé‡å¤
        if normalized_url in seen_urls:
            is_duplicate = True
            print(f"    âŒ URLé‡å¤: {title[:50]}...")
        
        # 2. æ£€æŸ¥æ ‡é¢˜é‡å¤ï¼ˆç›¸ä¼¼åº¦å¾ˆé«˜çš„æ ‡é¢˜ï¼‰
        elif title_hash in seen_title_hashes:
            is_duplicate = True
            print(f"    âŒ æ ‡é¢˜é‡å¤: {title[:50]}...")
        
        # 3. æ£€æŸ¥æ ‡é¢˜çš„é«˜åº¦ç›¸ä¼¼æ€§ï¼ˆæ›´ä¸¥æ ¼çš„æ£€æŸ¥ï¼‰
        else:
            for existing_article in deduplicated_articles:
                existing_title = existing_article.get('title', '').strip()
                
                # ç®€å•çš„ç›¸ä¼¼åº¦æ£€æŸ¥ï¼šå¦‚æœä¸¤ä¸ªæ ‡é¢˜æœ‰80%ä»¥ä¸Šçš„ç›¸åŒè¯æ±‡
                title_words = set(title.lower().split())
                existing_words = set(existing_title.lower().split())
                
                if title_words and existing_words:
                    intersection = len(title_words.intersection(existing_words))
                    union = len(title_words.union(existing_words))
                    similarity = intersection / union if union > 0 else 0
                    
                    if similarity > 0.8:  # 80%ç›¸ä¼¼åº¦é˜ˆå€¼
                        is_duplicate = True
                        print(f"    âŒ é«˜åº¦ç›¸ä¼¼æ ‡é¢˜: {title[:50]}...")
                        break
        
        if not is_duplicate:
            seen_urls.add(normalized_url)
            seen_title_hashes.add(title_hash)
            deduplicated_articles.append(article)
            
            # å¦‚æœå·²ç»è¾¾åˆ°æœ€å¤§æ–‡ç« æ•°ï¼Œåœæ­¢æ·»åŠ 
            if len(deduplicated_articles) >= max_articles:
                break
    
    removed_count = len(articles) - len(deduplicated_articles)
    print(f"  âœ… å»é‡å®Œæˆï¼Œç§»é™¤ {removed_count} ç¯‡é‡å¤æ–‡ç« ï¼Œä¿ç•™ {len(deduplicated_articles)} ç¯‡")
    
    return deduplicated_articles

def parse_cumulative_markdown(md_file_path, max_recent_articles=20):
    """
    è§£æç´¯ç§¯Markdownæ–‡ä»¶ï¼Œæå–æœ€è¿‘çš„æ–‡ç« ä¿¡æ¯ï¼Œå¹¶è¿›è¡Œå»é‡
    
    å‚æ•°:
        md_file_path (str): ç´¯ç§¯Markdownæ–‡ä»¶è·¯å¾„
        max_recent_articles (int): RSSä¸­åŒ…å«çš„æœ€å¤§æ–‡ç« æ•°
        
    è¿”å›:
        dict: åŒ…å«æ–°é—»ä¿¡æ¯çš„å­—å…¸
    """
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ–‡ä»¶ {md_file_path}: {e}")
        return None
    
    info = {
        'title': '',
        'description': '',
        'pub_date': '',
        'articles': []
    }
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'^# (.+)', content, re.MULTILINE)
    if title_match:
        info['title'] = title_match.group(1).strip()
    
    # æå–æœ€åæ›´æ–°æ—¶é—´
    time_match = re.search(r'\*\*æœ€åæ›´æ–°æ—¶é—´\*\*:\s*(.+)', content)
    if time_match:
        time_str = time_match.group(1).strip()
        try:
            pub_date = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
            info['pub_date'] = pub_date.strftime('%a, %d %b %Y %H:%M:%S GMT')
        except ValueError:
            info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    else:
        info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    # è®¾ç½®æè¿°
    info['description'] = "ç´¯ç§¯æ–°é—»æ±‡æ€»ï¼ŒæŒç»­æ›´æ–°çš„ç§‘æŠ€èµ„è®¯"
    
    # æå–æ–‡ç« ï¼ˆæå–æ›´å¤šæ–‡ç« ä»¥ä¾¿å»é‡åä»æœ‰è¶³å¤Ÿæ•°é‡ï¼‰
    # ä¸ºäº†ç¡®ä¿å»é‡åæœ‰è¶³å¤Ÿçš„æ–‡ç« ï¼Œæˆ‘ä»¬å…ˆæå–æ›´å¤šçš„æ–‡ç« 
    extraction_limit = max_recent_articles * 3  # æå–3å€æ•°é‡ç”¨äºå»é‡
    article_pattern = r'#### \[(.+?)\]\((.+?)\)\s*(?:\*\*å‘å¸ƒæ—¶é—´\*\*:\s*(.+?)(?:\n|$))?'
    articles = re.findall(article_pattern, content, re.MULTILINE | re.DOTALL)
    
    # é¦–å…ˆæå–æ›´å¤šæ–‡ç« 
    articles = articles[:extraction_limit]
    
    raw_articles = []
    for title, link, pub_time in articles:
        clean_title = title.replace('\\[', '[').replace('\\]', ']').strip()
        clean_link = link.strip()
        clean_pub_time = pub_time.strip() if pub_time else ''
        
        # è½¬æ¢å‘å¸ƒæ—¶é—´æ ¼å¼
        rss_pub_time = ''
        if clean_pub_time:
            try:
                for fmt in ['%Y-%m-%d %H:%M', '%a, %d %b %Y %H:%M:%S %z', '%Y-%m-%d %H:%M:%S']:
                    try:
                        dt = datetime.strptime(clean_pub_time, fmt)
                        rss_pub_time = dt.strftime('%a, %d %b %Y %H:%M:%S GMT')
                        break
                    except ValueError:
                        continue
            except:
                pass
        
        if not rss_pub_time:
            rss_pub_time = info['pub_date']
        
        raw_articles.append({
            'title': clean_title,
            'link': clean_link,
            'pub_date': rss_pub_time,
            'description': clean_title
        })
    
    # è¿›è¡Œå»é‡å¤„ç†
    info['articles'] = deduplicate_articles(raw_articles, max_recent_articles)
    
    return info

def get_original_rss_filename(category):
    """
    æ ¹æ®åˆ†ç±»è·å–åŸæœ‰çš„RSSæ–‡ä»¶å
    
    å‚æ•°:
        category (str): åˆ†ç±»åç§°
        
    è¿”å›:
        str: åŸæœ‰çš„RSSæ–‡ä»¶å
    """
    # å®šä¹‰åˆ†ç±»åˆ°åŸæœ‰æ–‡ä»¶åçš„æ˜ å°„
    category_filename_map = {
        'Finance': 'financefreenewsagent.xml',
        'finance': 'financefreenewsagent.xml',
        'Technology': 'technologyfreenewsagent.xml',
        'technology': 'technologyfreenewsagent.xml',
        'AI': 'aifreenewsagent.xml',
        'ai': 'aifreenewsagent.xml',
        'äººå·¥æ™ºèƒ½': 'aifreenewsagent.xml',
    }
    
    # é¦–å…ˆå°è¯•ç²¾ç¡®åŒ¹é…
    if category in category_filename_map:
        return category_filename_map[category]
    
    # ç„¶åå°è¯•å°å†™åŒ¹é…
    category_lower = category.lower()
    for key, filename in category_filename_map.items():
        if key.lower() == category_lower:
            return filename
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ˜ å°„ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
    safe_category = category.lower().replace(' ', '').replace('_', '').replace('-', '')
    return f"{safe_category}freenewsagent.xml"

def read_existing_rss_metadata(xml_file_path):
    """
    è¯»å–ç°æœ‰RSSæ–‡ä»¶çš„å…ƒæ•°æ®ï¼ˆå¦‚follow_challengeç­‰ï¼‰
    
    å‚æ•°:
        xml_file_path (str): RSSæ–‡ä»¶è·¯å¾„
        
    è¿”å›:
        dict: å…ƒæ•°æ®å­—å…¸
    """
    metadata = {}
    
    if not os.path.exists(xml_file_path):
        return metadata
    
    try:
        with open(xml_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–follow_challengeä¿¡æ¯
        follow_challenge_match = re.search(
            r'<follow_challenge>\s*<feedId>(\d+)</feedId>\s*<userId>([^<]+)</userId>\s*</follow_challenge>',
            content, re.MULTILINE | re.DOTALL
        )
        
        if follow_challenge_match:
            metadata['follow_challenge'] = {
                'feedId': follow_challenge_match.group(1),
                'userId': follow_challenge_match.group(2)
            }
    
    except Exception as e:
        print(f"  è¯»å–RSSå…ƒæ•°æ®æ—¶å‡ºé”™: {e}")
    
    return metadata

def get_category_follow_challenge(category):
    """
    æ ¹æ®åˆ†ç±»è·å–è®¤è¯ä¿¡æ¯
    
    å‚æ•°:
        category (str): åˆ†ç±»åç§°
        
    è¿”å›:
        dict: è®¤è¯ä¿¡æ¯å­—å…¸
    """
    # å®šä¹‰åˆ†ç±»å¯¹åº”çš„è®¤è¯ä¿¡æ¯
    follow_challenges = {
        'ai': {
            'feedId': '150782771375663104',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        },
        'technology': {
            'feedId': '150782874584542208',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        },
        'finance': {
            'feedId': '150806458867930112',
            'userId': 'DdasOQb1gouc5RwqkaQc4KLscHJhfeeW'
        }
    }
    
    category_lower = category.lower()
    
    # æ˜ å°„ä¸­æ–‡åç§°
    if category_lower in ['äººå·¥æ™ºèƒ½']:
        category_lower = 'ai'
    
    return follow_challenges.get(category_lower, None)

def generate_cumulative_rss_xml(news_info, category, base_url="https://zskksz.asia/News-Agent", 
                               existing_metadata=None):
    """
    ç”Ÿæˆç´¯ç§¯RSS XMLå†…å®¹ï¼Œä¿æŒåŸæœ‰çš„å…ƒæ•°æ®å’Œè®¤è¯ä¿¡æ¯
    
    å‚æ•°:
        news_info (dict): æ–°é—»ä¿¡æ¯
        category (str): åˆ†ç±»åç§°
        base_url (str): åŸºç¡€URL
        existing_metadata (dict): ç°æœ‰çš„å…ƒæ•°æ®
        
    è¿”å›:
        str: RSS XMLå­—ç¬¦ä¸²
    """
    rss = Element('rss')
    rss.set('version', '2.0')
    rss.set('xmlns:atom', 'http://www.w3.org/2005/Atom')
    
    channel = SubElement(rss, 'channel')
    
    # æ ¹æ®åˆ†ç±»è·å–åŸæœ‰æ–‡ä»¶å
    original_filename = get_original_rss_filename(category)
    
    title = SubElement(channel, 'title')
    title.text = f"{category} æ–°é—»æ±‡æ€» - Free News Agent"
    
    link = SubElement(channel, 'link')
    link.text = f"{base_url}/feed/{original_filename}"
    
    description = SubElement(channel, 'description')
    description.text = f"{category} åˆ†ç±»çš„æœ€æ–°æ–°é—»æ±‡æ€»ï¼Œç”± Free News Agent è‡ªåŠ¨ç”Ÿæˆ"
    
    language = SubElement(channel, 'language')
    language.text = "zh-CN"
    
    pub_date = SubElement(channel, 'pubDate')
    pub_date.text = news_info.get('pub_date', datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT'))
    
    last_build_date = SubElement(channel, 'lastBuildDate')
    last_build_date.text = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    generator = SubElement(channel, 'generator')
    generator.text = "News Agent RSS Generator"
    
    # æ·»åŠ è®¤è¯ä¿¡æ¯ - ä¼˜å…ˆä½¿ç”¨ç°æœ‰çš„ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤çš„
    follow_challenge_info = None
    if existing_metadata and 'follow_challenge' in existing_metadata:
        follow_challenge_info = existing_metadata['follow_challenge']
    else:
        follow_challenge_info = get_category_follow_challenge(category)
    
    if follow_challenge_info:
        follow_challenge = SubElement(channel, 'follow_challenge')
        feed_id = SubElement(follow_challenge, 'feedId')
        feed_id.text = follow_challenge_info['feedId']
        user_id = SubElement(follow_challenge, 'userId')
        user_id.text = follow_challenge_info['userId']
    
    # æ·»åŠ atom:linkè‡ªå¼•ç”¨
    atom_link = SubElement(channel, 'atom:link')
    atom_link.set('href', f"{base_url}/feed/{original_filename}")
    atom_link.set('rel', 'self')
    atom_link.set('type', 'application/rss+xml')
    
    # æ·»åŠ æ–‡ç« é¡¹ç›®
    for article in news_info.get('articles', []):
        item = SubElement(channel, 'item')
        
        item_title = SubElement(item, 'title')
        item_title.text = article['title']
        
        item_link = SubElement(item, 'link')
        item_link.text = article['link']
        
        item_description = SubElement(item, 'description')
        item_description.text = f"<![CDATA[{article['description']}]]>"
        
        item_pub_date = SubElement(item, 'pubDate')
        item_pub_date.text = article['pub_date']
        
        guid = SubElement(item, 'guid')
        guid.set('isPermaLink', 'true')
        guid.text = article['link']
        
        item_category = SubElement(item, 'category')
        item_category.text = category
    
    # æ ¼å¼åŒ–XML
    rough_string = tostring(rss, 'utf-8')
    reparsed = minidom.parseString(rough_string)
    return reparsed.toprettyxml(indent="  ", encoding='utf-8').decode('utf-8')

def get_cumulative_news_files(news_dir="cumulative_news"):
    """
    è·å–ç´¯ç§¯æ–°é—»æ–‡ä»¶åˆ—è¡¨
    """
    if not os.path.exists(news_dir):
        print(f"âŒ ç´¯ç§¯æ–°é—»ç›®å½•ä¸å­˜åœ¨: {news_dir}")
        return []
    
    news_files = []
    for filename in os.listdir(news_dir):
        if filename.endswith('_cumulative.md'):
            news_files.append(os.path.join(news_dir, filename))
    
    return news_files

def extract_category_from_cumulative_filename(filename):
    """
    ä»ç´¯ç§¯æ–‡ä»¶åä¸­æå–åˆ†ç±»åç§°
    """
    basename = os.path.basename(filename)
    category = basename.replace('_cumulative.md', '')
    return category

def main():
    """
    ä¸»å‡½æ•°ï¼šç”Ÿæˆç´¯ç§¯RSS Feedï¼Œç›´æ¥è¦†ç›–åŸæœ‰æ–‡ä»¶å¹¶ä¿ç•™è®¤è¯ä¿¡æ¯
    """
    print("=" * 60)
    print("ğŸ“¡ ç´¯ç§¯RSS Feedç”Ÿæˆå™¨ (è¦†ç›–åŸæœ‰æ–‡ä»¶ï¼Œä¿ç•™è®¤è¯)")
    print("=" * 60)
    
    # é…ç½®å‚æ•°
    news_dir = "cumulative_news"
    feed_dir = "feed"
    base_url = "https://zskksz.asia/News-Agent"
    max_articles_per_feed = 50  # æ¯ä¸ªRSS Feedæœ€å¤šåŒ…å«çš„æ–‡ç« æ•°
    
    print(f"ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  - ç´¯ç§¯æ–°é—»ç›®å½•: {news_dir}")
    print(f"  - Feedè¾“å‡ºç›®å½•: {feed_dir}")
    print(f"  - åŸºç¡€URL: {base_url}")
    print(f"  - æ¯Feedæœ€å¤§æ–‡ç« æ•°: {max_articles_per_feed}")
    print(f"  - æ¨¡å¼: è¦†ç›–åŸæœ‰RSSæ–‡ä»¶å¹¶ä¿ç•™è®¤è¯ä¿¡æ¯")
    print()
    
    # åˆ›å»ºfeedç›®å½•
    if not os.path.exists(feed_dir):
        os.makedirs(feed_dir)
        print(f"âœ… åˆ›å»ºFeedç›®å½•: {feed_dir}")
    
    # è·å–ç´¯ç§¯æ–°é—»æ–‡ä»¶
    news_files = get_cumulative_news_files(news_dir)
    
    if not news_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç´¯ç§¯æ–°é—»æ–‡ä»¶")
        return False
    
    print(f"ğŸ“° æ‰¾åˆ° {len(news_files)} ä¸ªç´¯ç§¯æ–°é—»æ–‡ä»¶")
    print("-" * 60)
    
    success_count = 0
    failed_count = 0
    
    # ä¸ºæ¯ä¸ªåˆ†ç±»ç”ŸæˆRSS Feed
    for news_file in news_files:
        category = extract_category_from_cumulative_filename(news_file)
        print(f"ğŸ”„ å¤„ç†åˆ†ç±»: {category}")
        
        try:
            # è§£æMarkdownæ–‡ä»¶
            news_info = parse_cumulative_markdown(news_file, max_articles_per_feed)
            
            if not news_info:
                print(f"  âŒ è§£æå¤±è´¥")
                failed_count += 1
                continue
            
            article_count = len(news_info.get('articles', []))
            print(f"  ğŸ“„ è§£æåˆ° {article_count} ç¯‡æ–‡ç« ")
            
            # è·å–åŸæœ‰RSSæ–‡ä»¶å
            original_filename = get_original_rss_filename(category)
            xml_path = os.path.join(feed_dir, original_filename)
            
            # è¯»å–ç°æœ‰RSSæ–‡ä»¶çš„å…ƒæ•°æ®
            existing_metadata = read_existing_rss_metadata(xml_path)
            if existing_metadata and 'follow_challenge' in existing_metadata:
                print(f"  ğŸ“‹ ä¿ç•™ç°æœ‰è®¤è¯ä¿¡æ¯: feedId={existing_metadata['follow_challenge']['feedId']}")
            else:
                default_auth = get_category_follow_challenge(category)
                if default_auth:
                    print(f"  ğŸ” ä½¿ç”¨é»˜è®¤è®¤è¯ä¿¡æ¯: feedId={default_auth['feedId']}")
                else:
                    print(f"  âš ï¸ æœªæ‰¾åˆ°è®¤è¯ä¿¡æ¯")
            
            # ç”ŸæˆRSS XML
            rss_xml = generate_cumulative_rss_xml(news_info, category, base_url, existing_metadata)
            
            # ç›´æ¥è¦†ç›–åŸæœ‰XMLæ–‡ä»¶
            with open(xml_path, 'w', encoding='utf-8') as f:
                f.write(rss_xml)
            
            print(f"  âœ… è¦†ç›–æ›´æ–°æˆåŠŸ: {original_filename}")
            success_count += 1
            
        except Exception as e:
            print(f"  âŒ ç”Ÿæˆå¤±è´¥: {e}")
            failed_count += 1
    
    # è¾“å‡ºç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆç»“æœç»Ÿè®¡:")
    print("=" * 60)
    print(f"  âœ… æˆåŠŸæ›´æ–°: {success_count} ä¸ªRSS Feed")
    print(f"  âŒ æ›´æ–°å¤±è´¥: {failed_count} ä¸ª")
    print(f"  ğŸ“ è¾“å‡ºç›®å½•: {os.path.abspath(feed_dir)}")
    print(f"  ğŸ”„ æ¨¡å¼: ç›´æ¥è¦†ç›–åŸæœ‰æ–‡ä»¶ï¼Œä¿ç•™è®¤è¯ä¿¡æ¯")
    
    if success_count > 0:
        print(f"\nğŸ“¡ æ›´æ–°çš„RSSè®¢é˜…åœ°å€ (æ— éœ€é‡æ–°è®¢é˜…):")
        for news_file in news_files:
            category = extract_category_from_cumulative_filename(news_file)
            original_filename = get_original_rss_filename(category)
            if os.path.exists(os.path.join(feed_dir, original_filename)):
                print(f"  - {category}: {base_url}/feed/{original_filename}")
    
    print(f"\nğŸ‰ RSS Feedæ›´æ–°å®Œæˆï¼ç”¨æˆ·æ— éœ€é‡æ–°è®¢é˜…ï¼")
    return success_count > 0

def show_help():
    """
    æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
    """
    print("=" * 60)
    print("ğŸ“– ç´¯ç§¯RSS Feedç”Ÿæˆå™¨")
    print("=" * 60)
    print()
    print("åŠŸèƒ½è¯´æ˜:")
    print("  - åŸºäºç´¯ç§¯æ–°é—»æ–‡æ¡£ç”ŸæˆRSS Feed")
    print("  - ç›´æ¥è¦†ç›–åŸæœ‰çš„RSSæ–‡ä»¶ï¼Œä¿æŒURLä¸å˜")
    print("  - ä¿ç•™åŸæœ‰çš„è®¤è¯ä¿¡æ¯ï¼ˆfollow_challengeï¼‰")
    print("  - æ”¯æŒä»ç°æœ‰æ–‡ä»¶è¯»å–æˆ–ä½¿ç”¨é»˜è®¤è®¤è¯ä¿¡æ¯")
    print("  - ç”¨æˆ·æ— éœ€é‡æ–°è®¢é˜…RSSæº")
    print()
    print("æ–‡ä»¶æ˜ å°„:")
    print("  - Finance -> financefreenewsagent.xml")
    print("  - Technology -> technologyfreenewsagent.xml") 
    print("  - AI -> aifreenewsagent.xml")
    print("  - å…¶ä»–åˆ†ç±» -> [åˆ†ç±»å]freenewsagent.xml")
    print()
    print("è®¤è¯ä¿¡æ¯:")
    print("  - ä¼˜å…ˆä½¿ç”¨ç°æœ‰RSSæ–‡ä»¶ä¸­çš„è®¤è¯ä¿¡æ¯")
    print("  - å¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é¢„å®šä¹‰çš„é»˜è®¤è®¤è¯ä¿¡æ¯")
    print("  - ç¡®ä¿RSS Feedçš„æ‰€æœ‰æƒéªŒè¯æ­£å¸¸")
    print()
    print("è¾“å…¥æ–‡ä»¶:")
    print("  - cumulative_news/åˆ†ç±»å_cumulative.md")
    print()
    print("è¾“å‡ºæ–‡ä»¶:")
    print("  - feed/åŸæœ‰æ–‡ä»¶å.xmlï¼ˆç›´æ¥è¦†ç›–ï¼‰")
    print()
    print("ä½¿ç”¨æ–¹æ³•:")
    print("  python ç”Ÿæˆç´¯ç§¯RSS.py")
    print("  python ç”Ÿæˆç´¯ç§¯RSS.py --help")

if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    if len(sys.argv) > 1 and sys.argv[1] in ['--help', '-h', 'help']:
        show_help()
    else:
        # æ‰§è¡Œä¸»ç¨‹åº
        success = main()
        
        if success:
            print("\nğŸ¯ ä»»åŠ¡å®Œæˆï¼")
        else:
            print("\nğŸ’¥ ä»»åŠ¡å¤±è´¥ï¼")
            sys.exit(1)