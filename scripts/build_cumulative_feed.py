"""
ç´¯ç§¯RSS Feedç”Ÿæˆè„šæœ¬

åŸºäºç´¯ç§¯æ–°é—»æ–‡æ¡£ç”ŸæˆRSS Feedï¼Œæ”¯æŒå¢é‡æ›´æ–°å’Œä¸¥æ ¼å»é‡
"""

import os
import sys
import re
import argparse
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from news_agent.config_loader import get_project_paths, load_config
from news_agent.history.rss_history import RSSHistoryManager
from news_agent.utils.deduplicate import create_content_fingerprint, calculate_title_similarity
from news_agent.rss.feed_generator import generate_rss_xml, read_existing_rss_metadata, get_rss_filename

try:
    from news_agent.filters.ai_news_filter import NewsQualityFilter
    AI_FILTER_AVAILABLE = True
except ImportError:
    print("âš ï¸ AIç­›é€‰æ¨¡å—ä¸å¯ç”¨ï¼Œå°†è·³è¿‡AIç­›é€‰åŠŸèƒ½")
    AI_FILTER_AVAILABLE = False


def parse_cumulative_markdown(md_file_path: str, category: str, 
                             history_manager: RSSHistoryManager,
                             max_recent_articles: int = 50,
                             time_window_hours: int = 72,
                             enable_ai_filter: bool = True,
                             ai_filter_count: int = 10) -> Dict:
    """
    è§£æç´¯ç§¯Markdownæ–‡ä»¶å¹¶æå–æ–‡ç« 
    
    å‚æ•°:
        md_file_path (str): Markdownæ–‡ä»¶è·¯å¾„
        category (str): åˆ†ç±»
        history_manager (RSSHistoryManager): å†å²ç®¡ç†å™¨
        max_recent_articles (int): æœ€å¤šæå–çš„æ–‡ç« æ•°
        time_window_hours (int): æ—¶é—´çª—å£ï¼ˆå°æ—¶ï¼‰
        enable_ai_filter (bool): æ˜¯å¦å¯ç”¨AIç­›é€‰
        ai_filter_count (int): AIç­›é€‰åä¿ç•™çš„æ–‡ç« æ•°
        
    è¿”å›:
        Dict: åŒ…å«æ–‡ç« åˆ—è¡¨çš„ä¿¡æ¯å­—å…¸
    """
    try:
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"  âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return {}
    
    info = {
        'title': '',
        'description': '',
        'pub_date': '',
        'articles': []
    }
    
    # æå–æ ‡é¢˜
    title_match = re.search(r'^# (.+)', content, re.MULTILINE)
    if title_match:
        info['title'] = title_match.group(1).strip()
    
    # æå–æœ€åæ›´æ–°æ—¶é—´
    time_match = re.search(r'\*\*æœ€åæ›´æ–°æ—¶é—´\*\*:\s*(.+)', content)
    if time_match:
        time_str = time_match.group(1).strip()
        try:
            pub_date = datetime.strptime(time_str, '%Y-%m-%d %H:%M')
            info['pub_date'] = pub_date.strftime('%a, %d %b %Y %H:%M:%S GMT')
        except ValueError:
            info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    else:
        info['pub_date'] = datetime.now().strftime('%a, %d %b %Y %H:%M:%S GMT')
    
    info['description'] = f"{category} åˆ†ç±»æœ€æ–°æ–°é—»ï¼Œå¢é‡æ›´æ–°ç¡®ä¿å†…å®¹æ–°é²œ"
    
    # æå–æ–‡ç« 
    extraction_limit = max_recent_articles * 10
    article_pattern = r'#### \[(.+?)\]\((.+?)\)\s*(?:\*\*å‘å¸ƒæ—¶é—´\*\*:\s*(.+?)(?:\n|$))?'
    articles = re.findall(article_pattern, content, re.MULTILINE | re.DOTALL)
    articles = articles[:extraction_limit]
    
    raw_articles = []
    time_cutoff = datetime.now() - timedelta(hours=time_window_hours)
    last_update_time = history_manager.get_last_update_time(category)
    
    for title, link, pub_time in articles:
        try:
            title_clean = title.replace('\\[', '[').replace('\\]', ']').strip()
            link_clean = link.strip()
            
            # è§£æå‘å¸ƒæ—¶é—´
            pub_datetime = None
            if pub_time:
                try:
                    pub_datetime = datetime.strptime(pub_time.strip(), '%Y-%m-%d %H:%M')
                except:
                    pass
            
            # æ—¶é—´è¿‡æ»¤
            if pub_datetime:
                if pub_datetime < time_cutoff:
                    continue
                if last_update_time and pub_datetime <= last_update_time:
                    continue
            
            # ç”ŸæˆæŒ‡çº¹
            fingerprint = create_content_fingerprint(title_clean, link_clean)
            
            # æ£€æŸ¥æ˜¯å¦å·²å‘å¸ƒ
            if history_manager.is_article_published(category, fingerprint):
                continue
            
            article_info = {
                'title': title_clean,
                'link': link_clean,
                'pub_date': pub_datetime.strftime('%a, %d %b %Y %H:%M:%S GMT') if pub_datetime else info['pub_date'],
                'description': title_clean,
                'fingerprint': fingerprint
            }
            
            raw_articles.append(article_info)
            
        except Exception as e:
            print(f"  âš ï¸ è§£ææ–‡ç« å¤±è´¥: {e}")
            continue
    
    # å»é‡ï¼šæ ‡é¢˜ç›¸ä¼¼åº¦æ£€æŸ¥
    deduplicated = []
    for article in raw_articles:
        is_duplicate = False
        for existing in deduplicated:
            similarity = calculate_title_similarity(article['title'], existing['title'])
            if similarity > 0.85:
                is_duplicate = True
                break
        
        if not is_duplicate:
            deduplicated.append(article)
    
    print(f"  ğŸ“Š æå– {len(raw_articles)} ç¯‡æ–°æ–‡ç« ï¼Œå»é‡å {len(deduplicated)} ç¯‡")
    
    # AIç­›é€‰
    if enable_ai_filter and AI_FILTER_AVAILABLE and len(deduplicated) > ai_filter_count:
        print(f"  ğŸ¤– å¯åŠ¨AIç­›é€‰: {len(deduplicated)} â†’ {ai_filter_count} ç¯‡")
        try:
            filter_instance = NewsQualityFilter()
            deduplicated = filter_instance.filter_articles(deduplicated, category, ai_filter_count)
        except Exception as e:
            print(f"  âš ï¸ AIç­›é€‰å¤±è´¥: {e}")
    
    # é™åˆ¶æ•°é‡
    deduplicated = deduplicated[:max_recent_articles]
    
    info['articles'] = deduplicated
    
    return info


def process_category(category: str, cumulative_file: Path, 
                    output_dir: Path, history_manager: RSSHistoryManager,
                    config: Dict) -> Dict:
    """
    å¤„ç†å•ä¸ªåˆ†ç±»çš„RSSç”Ÿæˆ
    
    å‚æ•°:
        category (str): åˆ†ç±»åç§°
        cumulative_file (Path): ç´¯ç§¯æ–°é—»æ–‡ä»¶è·¯å¾„
        output_dir (Path): è¾“å‡ºç›®å½•
        history_manager (RSSHistoryManager): å†å²ç®¡ç†å™¨
        config (Dict): é…ç½®å­—å…¸
        
    è¿”å›:
        Dict: å¤„ç†ç»“æœ
    """
    print(f"\nğŸ“° å¤„ç†åˆ†ç±»: {category}")
    print(f"  ğŸ“„ ç´¯ç§¯æ–‡ä»¶: {cumulative_file.name}")
    
    settings = config['settings']
    
    # è§£æMarkdown
    news_info = parse_cumulative_markdown(
        str(cumulative_file),
        category,
        history_manager,
        max_recent_articles=settings.get('max_articles_per_source', 50),
        time_window_hours=settings.get('time_window_hours', 72),
        enable_ai_filter=settings.get('ai_filter_enabled', True),
        ai_filter_count=settings.get('ai_filter_count', 10)
    )
    
    if not news_info or not news_info.get('articles'):
        print(f"  âš ï¸ æ²¡æœ‰æ–°æ–‡ç« ï¼Œè·³è¿‡")
        return {'success': False, 'reason': 'æ²¡æœ‰æ–°æ–‡ç« '}
    
    # ç”ŸæˆRSS XML
    rss_filename = get_rss_filename(category)
    rss_file_path = output_dir / rss_filename
    
    existing_metadata = read_existing_rss_metadata(str(rss_file_path))
    
    xml_content = generate_rss_xml(
        news_info, 
        category,
        base_url="https://zskksz.asia/News-Agent",
        existing_metadata=existing_metadata
    )
    
    # ä¿å­˜æ–‡ä»¶
    with open(rss_file_path, 'w', encoding='utf-8') as f:
        f.write(xml_content)
    
    # æ›´æ–°å†å²è®°å½•
    for article in news_info['articles']:
        history_manager.add_published_article(
            category,
            article['fingerprint'],
            article
        )
    
    history_manager.update_last_update_time(category)
    history_manager.save_history()
    
    print(f"  âœ… æˆåŠŸç”ŸæˆRSS: {rss_filename}")
    print(f"  ğŸ“Š åŒ…å« {len(news_info['articles'])} ç¯‡æ–°æ–‡ç« ")
    
    return {
        'success': True,
        'file': rss_filename,
        'article_count': len(news_info['articles'])
    }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='ç”Ÿæˆç´¯ç§¯RSS Feed')
    parser.add_argument('--category', type=str, help='æŒ‡å®šåˆ†ç±»ï¼ˆä¸æŒ‡å®šåˆ™å¤„ç†æ‰€æœ‰ï¼‰')
    parser.add_argument('--no-ai-filter', action='store_true', help='ç¦ç”¨AIç­›é€‰')
    parser.add_argument('--cleanup-days', type=int, default=30, help='æ¸…ç†å¤šå°‘å¤©å‰çš„å†å²è®°å½•')
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ“¡ ç´¯ç§¯RSS Feedç”Ÿæˆå™¨")
    print("=" * 60)
    
    # åŠ è½½é…ç½®
    config = load_config()
    paths = config['paths']
    
    if args.no_ai_filter:
        config['settings']['ai_filter_enabled'] = False
    
    print(f"\nğŸ“ è¾“å‡ºç›®å½•: {paths['feed']}")
    print(f"ğŸ“š ç´¯ç§¯æ–°é—»ç›®å½•: {paths['cumulative_news']}")
    
    # åˆå§‹åŒ–å†å²ç®¡ç†å™¨
    history_manager = RSSHistoryManager()
    
    # æ¸…ç†æ—§è®°å½•
    print(f"\nğŸ§¹ æ¸…ç† {args.cleanup_days} å¤©å‰çš„å†å²è®°å½•...")
    history_manager.cleanup_old_records(days=args.cleanup_days)
    
    # æŸ¥æ‰¾ç´¯ç§¯æ–°é—»æ–‡ä»¶
    cumulative_dir = Path(paths['cumulative_news'])
    if not cumulative_dir.exists():
        print(f"âŒ ç´¯ç§¯æ–°é—»ç›®å½•ä¸å­˜åœ¨: {cumulative_dir}")
        return
    
    cumulative_files = list(cumulative_dir.glob('*_cumulative.md'))
    
    if not cumulative_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ç´¯ç§¯æ–°é—»æ–‡ä»¶")
        return
    
    print(f"\nğŸ“‚ å‘ç° {len(cumulative_files)} ä¸ªç´¯ç§¯æ–°é—»æ–‡ä»¶")
    
    # å¤„ç†åˆ†ç±»
    results = {}
    for file_path in cumulative_files:
        # ä»æ–‡ä»¶åæå–åˆ†ç±»
        category = file_path.stem.replace('_cumulative', '').replace('_', ' ').title()
        
        # å¦‚æœæŒ‡å®šäº†åˆ†ç±»ï¼Œåªå¤„ç†è¯¥åˆ†ç±»
        if args.category and category.lower() != args.category.lower():
            continue
        
        result = process_category(
            category,
            file_path,
            Path(paths['feed']),
            history_manager,
            config
        )
        
        results[category] = result
    
    # è¾“å‡ºç»Ÿè®¡
    print("\n" + "=" * 60)
    print("ğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print("=" * 60)
    
    successful = [cat for cat, res in results.items() if res.get('success')]
    total_articles = sum(res.get('article_count', 0) for res in results.values())
    
    print(f"âœ… æˆåŠŸ: {len(successful)}/{len(results)} ä¸ªåˆ†ç±»")
    print(f"ğŸ“° æ€»æ–‡ç« æ•°: {total_articles}")
    
    for category, result in results.items():
        if result.get('success'):
            print(f"  âœ“ {category}: {result['article_count']} ç¯‡")
        else:
            print(f"  âœ— {category}: {result.get('reason', 'å¤±è´¥')}")
    
    print(f"\nğŸ‰ RSS Feedç”Ÿæˆå®Œæˆï¼")


if __name__ == "__main__":
    main()
