"""
å†…å®¹å»é‡å·¥å…·æ¨¡å—

æä¾›æ–‡ç« å»é‡ç›¸å…³çš„å·¥å…·å‡½æ•°
"""

import re
import hashlib
import difflib
from urllib.parse import urlparse
from typing import Tuple


def create_content_fingerprint(title: str, link: str, description: str = "") -> str:
    """
    åˆ›å»ºå†…å®¹æŒ‡çº¹ï¼Œç”¨äºç²¾ç¡®å»é‡
    
    å‚æ•°:
        title (str): æ–‡ç« æ ‡é¢˜
        link (str): æ–‡ç« é“¾æ¥
        description (str): æ–‡ç« æè¿°ï¼ˆå¯é€‰ï¼‰
        
    è¿”å›:
        str: å†…å®¹æŒ‡çº¹ï¼ˆSHA256å“ˆå¸Œå€¼ï¼‰
    """
    # æ¸…ç†æ ‡é¢˜ï¼šç§»é™¤ç‰¹æ®Šå­—ç¬¦ã€æ ‡ç‚¹ç¬¦å·ï¼Œè½¬æ¢ä¸ºå°å†™
    clean_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
    clean_title = ' '.join(clean_title.split())  # æ ‡å‡†åŒ–ç©ºæ ¼
    
    # æ¸…ç†é“¾æ¥ï¼šç§»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µ
    parsed_url = urlparse(link)
    clean_link = f"{parsed_url.netloc}{parsed_url.path}".lower()
    
    # ç”Ÿæˆç»„åˆæŒ‡çº¹
    content = f"{clean_title}|{clean_link}|{description[:100]}"
    return hashlib.sha256(content.encode('utf-8')).hexdigest()


def calculate_title_similarity(title1: str, title2: str) -> float:
    """
    è®¡ç®—ä¸¤ä¸ªæ ‡é¢˜çš„ç›¸ä¼¼åº¦
    
    å‚æ•°:
        title1 (str): æ ‡é¢˜1
        title2 (str): æ ‡é¢˜2
        
    è¿”å›:
        float: ç›¸ä¼¼åº¦ (0-1)
    """
    # æ¸…ç†å’Œæ ‡å‡†åŒ–æ ‡é¢˜
    def clean_title(title):
        # ç§»é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Šå­—ç¬¦
        cleaned = re.sub(r'[^\w\s]', ' ', title.lower())
        # æ ‡å‡†åŒ–ç©ºæ ¼
        return ' '.join(cleaned.split())
    
    cleaned_title1 = clean_title(title1)
    cleaned_title2 = clean_title(title2)
    
    # ä½¿ç”¨difflibè®¡ç®—ç›¸ä¼¼åº¦
    similarity = difflib.SequenceMatcher(None, cleaned_title1, cleaned_title2).ratio()
    
    # åŒæ—¶æ£€æŸ¥è¯æ±‡é‡å åº¦
    words1 = set(cleaned_title1.split())
    words2 = set(cleaned_title2.split())
    
    if words1 and words2:
        word_overlap = len(words1.intersection(words2)) / len(words1.union(words2))
        # å–ä¸¤ç§æ–¹æ³•çš„æœ€å¤§å€¼
        similarity = max(similarity, word_overlap)
    
    return similarity


def generate_article_hash(title: str, link: str) -> str:
    """
    ç”Ÿæˆæ–‡ç« çš„å”¯ä¸€å“ˆå¸Œå€¼ï¼ˆç®€åŒ–ç‰ˆæŒ‡çº¹ï¼‰
    
    å‚æ•°:
        title (str): æ–‡ç« æ ‡é¢˜
        link (str): æ–‡ç« é“¾æ¥
        
    è¿”å›:
        str: æ–‡ç« çš„MD5å“ˆå¸Œå€¼
    """
    content = f"{title.strip()}{link.strip()}"
    return hashlib.md5(content.encode('utf-8')).hexdigest()


def normalize_url(url: str) -> str:
    """
    æ ‡å‡†åŒ–URLï¼Œç§»é™¤æŸ¥è¯¢å‚æ•°å’Œç‰‡æ®µ
    
    å‚æ•°:
        url (str): åŸå§‹URL
        
    è¿”å›:
        str: æ ‡å‡†åŒ–åçš„URL
    """
    parsed = urlparse(url)
    return f"{parsed.scheme}://{parsed.netloc}{parsed.path}".lower()


def extract_domain(url: str) -> str:
    """
    ä»URLä¸­æå–åŸŸå
    
    å‚æ•°:
        url (str): URL
        
    è¿”å›:
        str: åŸŸå
    """
    parsed = urlparse(url)
    return parsed.netloc.lower()


if __name__ == "__main__":
    # æµ‹è¯•å»é‡å‡½æ•°
    print("ğŸ§ª æµ‹è¯•å»é‡å·¥å…·...")
    
    title1 = "GPT-5 Released: A Major Breakthrough in AI"
    title2 = "GPT 5 Released A Major Breakthrough in AI"
    link1 = "https://example.com/article/123?utm_source=rss"
    link2 = "https://example.com/article/123"
    
    print(f"\næ ‡é¢˜1: {title1}")
    print(f"æ ‡é¢˜2: {title2}")
    print(f"ç›¸ä¼¼åº¦: {calculate_title_similarity(title1, title2):.2%}")
    
    print(f"\nURL1: {link1}")
    print(f"URL2: {link2}")
    print(f"æ ‡å‡†åŒ–URL1: {normalize_url(link1)}")
    print(f"æ ‡å‡†åŒ–URL2: {normalize_url(link2)}")
    
    fp1 = create_content_fingerprint(title1, link1)
    fp2 = create_content_fingerprint(title2, link2)
    print(f"\næŒ‡çº¹1: {fp1[:16]}...")
    print(f"æŒ‡çº¹2: {fp2[:16]}...")
    print(f"æŒ‡çº¹ç›¸åŒ: {fp1 == fp2}")
